{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "78558224",
   "metadata": {},
   "source": [
    "# DataLoader Tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8cc03ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "sys.path.append('./lib')\n",
    "\n",
    "from dataloader import create_dataloader\n",
    "import torch\n",
    "import tiktoken\n",
    "tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
    "\n",
    "# Test text\n",
    "sample_text = \"\"\"\n",
    "In the world of artificial intelligence, language models have revolutionized how we process and generate text. \n",
    "These systems use architectures like transformers to learn complex patterns in textual data.\n",
    "The tokenization process converts text into numbers that the model can process efficiently.\n",
    "DataLoaders allow loading and processing data in batches during training.\n",
    "The sliding window technique with stride helps create overlapping sequences for better learning.\n",
    "\"\"\"\n",
    "\n",
    "print(\"=== Testing DataLoader with different stride values ===\\n\")\n",
    "\n",
    "# Base configuration\n",
    "batch_size = 2\n",
    "max_length = 20\n",
    "\n",
    "# Test with different stride values\n",
    "strides = [5]\n",
    "\n",
    "for stride in strides:\n",
    "    print(f\"--- STRIDE = {stride} ---\")\n",
    "    \n",
    "    # Create DataLoader\n",
    "    dataloader = create_dataloader(\n",
    "        text=sample_text,\n",
    "        batch_size=batch_size,\n",
    "        max_length=max_length,\n",
    "        stride=stride,\n",
    "        shuffle=False \n",
    "    )\n",
    "    \n",
    "    print(f\"Dataset size: {len(dataloader.dataset)} sequences\")\n",
    "    print(f\"Number of batches: {len(dataloader)}\")\n",
    "    \n",
    "    # Show first 2 batches\n",
    "    for i, (inputs, targets) in enumerate(dataloader):\n",
    "        if i >= 2:  # Only show first 2 batches\n",
    "            break\n",
    "        print(f\"Batch {i}:\")\n",
    "        print(f\"  Input shape: {inputs.shape}\")\n",
    "        print(f\"  Input tokens (first seq): {inputs[0][:30].tolist()}...\")\n",
    "        print(f\"  Target tokens (first seq): {targets[0][:30].tolist()}...\")\n",
    "        print(f\"  Target text (first seq): {tokenizer.decode(targets[0].tolist())}\")\n",
    "        print(f\"  Input tokens (second seq): {inputs[1][:10].tolist()}...\")\n",
    "        print(f\"  Target tokens (second seq): {targets[1][:10].tolist()}...\")\n",
    "        print()\n",
    "    \n",
    "    print(f\"{'='*50}\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "3.12.11",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
