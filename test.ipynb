{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "78558224",
   "metadata": {},
   "source": [
    "# DataLoader Tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a8cc03ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Testing DataLoader with different stride values ===\n",
      "\n",
      "--- STRIDE = 1 ---\n",
      "Dataset size: 63 sequences\n",
      "Number of batches: 31\n",
      "Batch 0:\n",
      "  Input shape: torch.Size([2, 20])\n",
      "  Input tokens (first seq): [198, 818, 262, 995, 286, 11666, 4430, 11, 3303, 4981, 423, 5854, 1143, 703, 356, 1429, 290, 7716, 2420, 13]...\n",
      "  Target tokens (first seq): [818, 262, 995, 286, 11666, 4430, 11, 3303, 4981, 423, 5854, 1143, 703, 356, 1429, 290, 7716, 2420, 13, 220]...\n",
      "  Target text (first seq): In the world of artificial intelligence, language models have revolutionized how we process and generate text. \n",
      "  Input tokens (second seq): [818, 262, 995, 286, 11666, 4430, 11, 3303, 4981, 423]...\n",
      "  Target tokens (second seq): [262, 995, 286, 11666, 4430, 11, 3303, 4981, 423, 5854]...\n",
      "\n",
      "Batch 1:\n",
      "  Input shape: torch.Size([2, 20])\n",
      "  Input tokens (first seq): [262, 995, 286, 11666, 4430, 11, 3303, 4981, 423, 5854, 1143, 703, 356, 1429, 290, 7716, 2420, 13, 220, 198]...\n",
      "  Target tokens (first seq): [995, 286, 11666, 4430, 11, 3303, 4981, 423, 5854, 1143, 703, 356, 1429, 290, 7716, 2420, 13, 220, 198, 4711]...\n",
      "  Target text (first seq):  world of artificial intelligence, language models have revolutionized how we process and generate text. \n",
      "These\n",
      "  Input tokens (second seq): [995, 286, 11666, 4430, 11, 3303, 4981, 423, 5854, 1143]...\n",
      "  Target tokens (second seq): [286, 11666, 4430, 11, 3303, 4981, 423, 5854, 1143, 703]...\n",
      "\n",
      "--- STRIDE = 5 ---\n",
      "Dataset size: 13 sequences\n",
      "Number of batches: 6\n",
      "Batch 0:\n",
      "  Input shape: torch.Size([2, 20])\n",
      "  Input tokens (first seq): [198, 818, 262, 995, 286, 11666, 4430, 11, 3303, 4981, 423, 5854, 1143, 703, 356, 1429, 290, 7716, 2420, 13]...\n",
      "  Target tokens (first seq): [818, 262, 995, 286, 11666, 4430, 11, 3303, 4981, 423, 5854, 1143, 703, 356, 1429, 290, 7716, 2420, 13, 220]...\n",
      "  Target text (first seq): In the world of artificial intelligence, language models have revolutionized how we process and generate text. \n",
      "  Input tokens (second seq): [11666, 4430, 11, 3303, 4981, 423, 5854, 1143, 703, 356]...\n",
      "  Target tokens (second seq): [4430, 11, 3303, 4981, 423, 5854, 1143, 703, 356, 1429]...\n",
      "\n",
      "Batch 1:\n",
      "  Input shape: torch.Size([2, 20])\n",
      "  Input tokens (first seq): [423, 5854, 1143, 703, 356, 1429, 290, 7716, 2420, 13, 220, 198, 4711, 3341, 779, 45619, 588, 6121, 364, 284]...\n",
      "  Target tokens (first seq): [5854, 1143, 703, 356, 1429, 290, 7716, 2420, 13, 220, 198, 4711, 3341, 779, 45619, 588, 6121, 364, 284, 2193]...\n",
      "  Target text (first seq):  revolutionized how we process and generate text. \n",
      "These systems use architectures like transformers to learn\n",
      "  Input tokens (second seq): [1429, 290, 7716, 2420, 13, 220, 198, 4711, 3341, 779]...\n",
      "  Target tokens (second seq): [290, 7716, 2420, 13, 220, 198, 4711, 3341, 779, 45619]...\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "sys.path.append('./lib')\n",
    "\n",
    "from dataloader import create_dataloader\n",
    "import torch\n",
    "import tiktoken\n",
    "tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
    "\n",
    "# Test text\n",
    "sample_text = \"\"\"\n",
    "In the world of artificial intelligence, language models have revolutionized how we process and generate text. \n",
    "These systems use architectures like transformers to learn complex patterns in textual data.\n",
    "The tokenization process converts text into numbers that the model can process efficiently.\n",
    "DataLoaders allow loading and processing data in batches during training.\n",
    "The sliding window technique with stride helps create overlapping sequences for better learning.\n",
    "\"\"\"\n",
    "\n",
    "print(\"=== Testing DataLoader with different stride values ===\\n\")\n",
    "\n",
    "# Base configuration\n",
    "batch_size = 2\n",
    "max_length = 20\n",
    "\n",
    "# Test with different stride values\n",
    "strides = [1,5]\n",
    "\n",
    "for stride in strides:\n",
    "    print(f\"--- STRIDE = {stride} ---\")\n",
    "    \n",
    "    # Create DataLoader\n",
    "    dataloader = create_dataloader(\n",
    "        text=sample_text,\n",
    "        batch_size=batch_size,\n",
    "        max_length=max_length,\n",
    "        stride=stride,\n",
    "        shuffle=False \n",
    "    )\n",
    "    \n",
    "    print(f\"Dataset size: {len(dataloader.dataset)} sequences\")\n",
    "\n",
    "    print(f\"Number of batches: {len(dataloader)}\")\n",
    "    \n",
    "    # Show first 2 batches\n",
    "    for i, (inputs, targets) in enumerate(dataloader):\n",
    "        if i >= 2:  # Only show first 2 batches\n",
    "            break\n",
    "        print(f\"Batch {i}:\")\n",
    "        print(f\"  Input shape: {inputs.shape}\")\n",
    "        print(f\"  Input tokens (first seq): {inputs[0][:30].tolist()}...\")\n",
    "        print(f\"  Target tokens (first seq): {targets[0][:30].tolist()}...\")\n",
    "        print(f\"  Target text (first seq): {tokenizer.decode(targets[0].tolist())}\")\n",
    "        print(f\"  Input tokens (second seq): {inputs[1][:10].tolist()}...\")\n",
    "        print(f\"  Target tokens (second seq): {targets[1][:10].tolist()}...\")\n",
    "        print()\n",
    "    \n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4d1917c",
   "metadata": {},
   "outputs": [],
   "source": [
    "GPT_CONFIG_124M = {\n",
    "    \"vocab_size\": 50257,   # Vocabulary size\n",
    "    \"context_length\": 256, # Shortened context length (orig: 1024)\n",
    "    \"emb_dim\": 768,        # Embedding dimension\n",
    "    \"n_heads\": 12,         # Number of attention heads\n",
    "    \"n_layers\": 12,        # Number of layers\n",
    "    \"drop_rate\": 0.1,      # Dropout rate\n",
    "    \"qkv_bias\": False      # Query-key-value bias\n",
    "}\n",
    "\n",
    "\n",
    "tok_emb = torch.nn.Embedding(GPT_CONFIG_124M[\"vocab_size\"], GPT_CONFIG_124M[\"emb_dim\"])\n",
    "pos_emb = torch.nn.Embedding(GPT_CONFIG_124M[\"context_length\"], GPT_CONFIG_124M[\"emb_dim\"])\n",
    "drop_emb = torch.nn.Dropout(GPT_CONFIG_124M[\"drop_rate\"])\n",
    "\n",
    "\n",
    "for i, (inputs, targets) in enumerate(dataloader):\n",
    "    if i >= 2:  \n",
    "        break\n",
    "\n",
    "    batch_size, seq_len = inputs.shape\n",
    "    print(f\"--- Batch {i} ---\")\n",
    "    print(f\"Batch size: {batch_size}\")\n",
    "    print(f\"Sequence length: {seq_len}\")\n",
    "\n",
    "    tok_embeds = tok_emb(inputs)\n",
    "    print(f\"Token embeddings shape: {tok_embeds.shape}\")\n",
    "    print(tok_embeds)\n",
    "\n",
    "    pos_indices = torch.arange(seq_len)\n",
    "    print(f\"Position indices: {pos_indices}\")\n",
    "    pos_embeds = pos_emb(pos_indices)\n",
    "    print(f\"Position embeddings shape: {pos_embeds.shape}\")\n",
    "    print(pos_embeds)\n",
    "\n",
    "    final_embeds = tok_embeds + pos_embeds\n",
    "    result = drop_emb(final_embeds)\n",
    "    print(f\"Final result shape: {result.shape}\")\n",
    "    print(result)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f3a6150",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, d_in, d_out, context_length, dropout, num_heads, qkv_bias=False):\n",
    "        super().__init__()\n",
    "        assert d_out % num_heads == 0, \"d_out must be divisible by n_heads\"\n",
    "\n",
    "        self.d_out = d_out\n",
    "        self.num_heads = num_heads\n",
    "        self.head_dim = d_out // num_heads  # Reduce the projection dim to match desired output dim\n",
    "\n",
    "        self.W_query = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "        self.W_key = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "        self.W_value = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "        self.out_proj = nn.Linear(d_out, d_out)  # Linear layer to combine head outputs\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.register_buffer('mask', torch.triu(torch.ones(context_length, context_length), diagonal=1))\n",
    "\n",
    "    def forward(self, x):\n",
    "        print(\"=\" * 50)\n",
    "        print(\"MULTIHEAD ATTENTION FORWARD PASS\")\n",
    "        print(\"=\" * 50)\n",
    "        print(\"W_key weight matrix:\")\n",
    "        print(self.W_key.weight)\n",
    "        \n",
    "        b, num_tokens, d_in = x.shape\n",
    "        print(f\"INPUT: batch_size={b}, num_tokens={num_tokens}, d_in={d_in}\")\n",
    "        print(f\"Input shape: {x.shape}\")\n",
    "        print(f\"Configuration: d_out={self.d_out}, num_heads={self.num_heads}, head_dim={self.head_dim}\")\n",
    "        print()\n",
    "\n",
    "        # Linear transformations\n",
    "        print(\"1. LINEAR TRANSFORMATIONS (Q, K, V)\")\n",
    "        print(\"-\" * 30)\n",
    "        print(\"W_key weight matrix:\")\n",
    "        print(self.W_key.weight)\n",
    "        keys = self.W_key(x)  # Shape: (b, num_tokens, d_out)\n",
    "        queries = self.W_query(x)\n",
    "        values = self.W_value(x)\n",
    "        print(f\"After linear layers:\")\n",
    "        print(f\"  Keys shape: {keys.shape}\")\n",
    "        print(f\"  Keys tensor:\")\n",
    "        print(keys)\n",
    "        print(f\"  Queries shape: {queries.shape}\")\n",
    "        print(f\"  Values shape: {values.shape}\")\n",
    "        print()\n",
    "\n",
    "        # We implicitly split the matrix by adding a `num_heads` dimension\n",
    "        # Unroll last dim: (b, num_tokens, d_out) -> (b, num_tokens, num_heads, head_dim)\n",
    "        print(\"2. VIEW OPERATION - SPLIT INTO HEADS\")\n",
    "        print(\"-\" * 30)\n",
    "        print(f\"Reshaping from (b, num_tokens, d_out) to (b, num_tokens, num_heads, head_dim)\")\n",
    "        print(f\"  d_out={self.d_out} = num_heads={self.num_heads} Ã— head_dim={self.head_dim}\")\n",
    "        \n",
    "        keys = keys.view(b, num_tokens, self.num_heads, self.head_dim)\n",
    "        values = values.view(b, num_tokens, self.num_heads, self.head_dim)\n",
    "        queries = queries.view(b, num_tokens, self.num_heads, self.head_dim)\n",
    "        \n",
    "        print(f\"After view operation:\")\n",
    "        print(f\"  Keys shape: {keys.shape}\")\n",
    "        print(f\"  Keys AW: {keys}\")\n",
    "        print(f\"  Queries shape: {queries.shape}\")\n",
    "        print(f\"  Values shape: {values.shape}\")\n",
    "        print()\n",
    "\n",
    "        # Transpose: (b, num_tokens, num_heads, head_dim) -> (b, num_heads, num_tokens, head_dim)\n",
    "        print(\"3. TRANSPOSE OPERATION\")\n",
    "        print(\"-\" * 30)\n",
    "        print(f\"Transposing dimensions 1 and 2:\")\n",
    "        print(f\"  (b, num_tokens, num_heads, head_dim) -> (b, num_heads, num_tokens, head_dim)\")\n",
    "        \n",
    "        keys = keys.transpose(1, 2)\n",
    "        queries = queries.transpose(1, 2)\n",
    "        values = values.transpose(1, 2)\n",
    "        \n",
    "        print(f\"After transpose:\")\n",
    "        print(f\"  Keys shape: {keys.shape}\")\n",
    "        print(f\"  Keys AT: {keys}\")\n",
    "        print(f\"  Queries shape: {queries.shape}\")\n",
    "        print(f\"  Values shape: {values.shape}\")\n",
    "        print()\n",
    "\n",
    "        # Compute scaled dot-product attention (aka self-attention) with a causal mask\n",
    "        print(\"4. ATTENTION SCORES COMPUTATION\")\n",
    "        print(\"-\" * 30)\n",
    "        print(f\"Computing Q @ K^T for each head:\")\n",
    "        print(f\"  queries shape: {queries.shape}\")\n",
    "        print(f\"  keys.transpose(2,3) shape: {keys.transpose(2, 3).shape}\")\n",
    "        \n",
    "        attn_scores = queries @ keys.transpose(2, 3)  # Dot product for each head\n",
    "        print(f\"Attention scores shape: {attn_scores.shape}\")\n",
    "        print(f\"Scaling factor (sqrt(head_dim)): {keys.shape[-1]**0.5}\")\n",
    "        print()\n",
    "\n",
    "        # Original mask truncated to the number of tokens and converted to boolean\n",
    "        print(\"5. CAUSAL MASK APPLICATION\")\n",
    "        print(\"-\" * 30)\n",
    "        mask_bool = self.mask.bool()[:num_tokens, :num_tokens]\n",
    "        print(f\"Mask shape: {mask_bool.shape}\")\n",
    "        print(f\"Mask (True=masked, False=allowed):\")\n",
    "        print(mask_bool)\n",
    "        print()\n",
    "\n",
    "        # Use the mask to fill attention scores\n",
    "        print(\"Before masking - attention scores (first head of first batch):\")\n",
    "        print(attn_scores[0, 0])\n",
    "        attn_scores.masked_fill_(mask_bool, -torch.inf)\n",
    "        print(\"After masking - attention scores (first head of first batch):\")\n",
    "        print(attn_scores[0, 0])\n",
    "        print()\n",
    "\n",
    "        print(\"6. SOFTMAX AND DROPOUT\")\n",
    "        print(\"-\" * 30)\n",
    "        attn_weights = torch.softmax(attn_scores / keys.shape[-1]**0.5, dim=-1)\n",
    "        print(f\"Attention weights shape: {attn_weights.shape}\")\n",
    "        print(\"Attention weights (first head of first batch):\")\n",
    "        print(attn_weights[0, 0])\n",
    "        \n",
    "        attn_weights = self.dropout(attn_weights)\n",
    "        print(\"After dropout applied\")\n",
    "        print()\n",
    "\n",
    "        # Shape: (b, num_tokens, num_heads, head_dim)\n",
    "        print(\"7. WEIGHTED VALUES COMPUTATION\")\n",
    "        print(\"-\" * 30)\n",
    "        print(f\"Computing attention_weights @ values:\")\n",
    "        print(f\"  attention_weights shape: {attn_weights.shape}\")\n",
    "        print(f\"  values shape: {values.shape}\")\n",
    "        \n",
    "        context_vec = (attn_weights @ values).transpose(1, 2)\n",
    "        print(f\"Context vector after transpose: {context_vec.shape}\")\n",
    "        print()\n",
    "\n",
    "        # Combine heads, where self.d_out = self.num_heads * self.head_dim\n",
    "        print(\"8. COMBINE HEADS (RESHAPE)\")\n",
    "        print(\"-\" * 30)\n",
    "        print(f\"Reshaping from {context_vec.shape} to (b, num_tokens, d_out)\")\n",
    "        context_vec = context_vec.reshape(b, num_tokens, self.d_out)\n",
    "        print(f\"Combined heads shape: {context_vec.shape}\")\n",
    "        \n",
    "        print(\"9. OUTPUT PROJECTION\")\n",
    "        print(\"-\" * 30)\n",
    "        context_vec = self.out_proj(context_vec)  # optional projection\n",
    "        print(f\"Final output shape: {context_vec.shape}\")\n",
    "        print()\n",
    "\n",
    "        return context_vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c3511a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import sys\n",
    "# import os\n",
    "# sys.path.append('./lib')\n",
    "# from multihead_attention import MultiHeadAttention\n",
    "# import torch\n",
    "\n",
    "# Create example with specified parameters\n",
    "mha = MultiHeadAttention(\n",
    "    d_in=6,\n",
    "    d_out=6,\n",
    "    context_length=4,\n",
    "    num_heads=3,\n",
    "    dropout=0.1,\n",
    "    qkv_bias=False\n",
    ")\n",
    "\n",
    "torch.manual_seed(42)\n",
    "# Create sample input data (batch_size=2, sequence_length=4, d_in=6)\n",
    "sample_input = torch.tensor([\n",
    "    # Batch 1\n",
    "    [\n",
    "        [1.0, 2.0, 3.0, 4.0, 5.0, 6.0],    # Token 1\n",
    "        [0.5, 1.5, 2.5, 3.5, 4.5, 5.5],    # Token 2\n",
    "        [2.0, 4.0, 6.0, 8.0, 10.0, 12.0],  # Token 3\n",
    "        [1.0, 0.0, -1.0, 2.0, 3.0, -2.0]   # Token 4\n",
    "    ],\n",
    "    # Batch 2\n",
    "    [\n",
    "        [0.1, 0.2, 0.3, 0.4, 0.5, 0.6],    # Token 1\n",
    "        [1.1, 1.2, 1.3, 1.4, 1.5, 1.6],    # Token 2\n",
    "        [-1.0, 2.0, -3.0, 4.0, -5.0, 6.0], # Token 3\n",
    "        [0.0, 1.0, 0.0, 1.0, 0.0, 1.0]     # Token 4\n",
    "    ]\n",
    "])\n",
    "\n",
    "print(\"Input shape:\", sample_input.shape)\n",
    "print(\"Input tensor:\")\n",
    "print(sample_input)\n",
    "print()\n",
    "\n",
    "# Forward pass through MultiHeadAttention\n",
    "output = mha(sample_input)\n",
    "print(\"Output shape:\", output.shape)\n",
    "print(\"Output tensor:\")\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "c4d55fac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting GPT test...\n",
      "Creating GPT model...\n",
      "Model created successfully!\n",
      "Creating dataloader...\n",
      "Dataloader created! Dataset size: 33\n",
      "Number of batches: 33\n",
      "Starting forward pass...\n",
      "Processing Batch 0:\n",
      "Input shape: torch.Size([1, 50])\n",
      "prompt_str \n",
      "In the world of artificial intelligence, language models have revolutionized how we process and generate text. \n",
      "These systems use architectures like transformers to learn complex patterns in textual data.\n",
      "The tokenization process converts text into numbers that the model can\n",
      "in_idx.shape() torch.Size([1, 50])\n",
      "logits: tensor([[[ 0.7509,  0.1538,  0.0703,  ..., -0.5794, -0.5429, -0.3805],\n",
      "         [-0.0838, -0.4568, -0.1122,  ...,  0.2135, -1.6031,  0.9259],\n",
      "         [ 0.2893,  0.4958, -0.4716,  ..., -0.2217, -1.2560,  0.8927],\n",
      "         ...,\n",
      "         [ 0.0173,  0.6606, -0.1656,  ..., -0.3013, -0.9760, -0.1228],\n",
      "         [ 0.1959,  0.0796,  0.5696,  ..., -0.0725, -0.7797,  0.0600],\n",
      "         [-0.5975,  0.0665,  0.3491,  ...,  0.2148, -0.7986,  0.2971]]],\n",
      "       grad_fn=<UnsafeViewBackward0>)\n",
      "torch.Size([1, 50, 50257])\n",
      "predicted_token_id tensor([34983, 18738, 49889,  4811, 21080, 21395, 49102, 29207, 42043, 34024,\n",
      "        35817, 33475, 20163, 39309, 36818, 12375, 37948, 17686, 31501,  7483,\n",
      "         7236, 10114, 29671, 20123, 37695,  6883, 17643, 48009, 48078, 12684,\n",
      "         2954, 20803, 19809, 46048,  2378,  3683, 14229, 27181, 23041,   522,\n",
      "        46265, 11284, 38377, 39649, 49092, 48237,   212, 46168, 29851, 47531])\n",
      "\n",
      "In the world of artificial intelligence, language models have revolutionized how we process and generate text. \n",
      "These systems use architectures like transformers to learn complex patterns in textual data.\n",
      "The tokenization process converts text into numbers that the model can Bitcoins\n",
      "Success! Output shape: torch.Size([1, 50, 50257])\n",
      "Sample logits: tensor([ 0.7509,  0.1538,  0.0703, -0.8301, -0.6692], grad_fn=<SliceBackward0>)\n",
      "\n",
      "Processing Batch 1:\n",
      "Input shape: torch.Size([1, 50])\n",
      "prompt_str In the world of artificial intelligence, language models have revolutionized how we process and generate text. \n",
      "These systems use architectures like transformers to learn complex patterns in textual data.\n",
      "The tokenization process converts text into numbers that the model can process\n",
      "in_idx.shape() torch.Size([1, 50])\n",
      "logits: tensor([[[ 0.6738,  0.1613, -0.7650,  ...,  0.5522, -0.2843,  0.9716],\n",
      "         [ 0.9086, -0.0796, -0.4594,  ..., -0.3356, -0.7297,  0.1098],\n",
      "         [-0.6814,  1.4966,  0.4794,  ...,  0.8314, -1.2572,  0.7060],\n",
      "         ...,\n",
      "         [-0.4223,  0.2475,  0.1502,  ...,  0.4753, -1.1347, -0.3550],\n",
      "         [-0.1765,  0.3630,  0.7183,  ..., -0.9848,  0.1761,  0.1609],\n",
      "         [-0.3264,  0.2482, -0.7215,  ...,  0.7421, -1.2716, -0.1247]]],\n",
      "       grad_fn=<UnsafeViewBackward0>)\n",
      "torch.Size([1, 50, 50257])\n",
      "predicted_token_id tensor([10567, 38815, 33333,  2265, 25706, 35070, 14740, 14490, 31604, 31468,\n",
      "        40079, 18676, 35679, 33477, 20989,  8464, 21830, 39177, 32097, 29282,\n",
      "        15416,  1774, 28505, 26460, 16842, 48045,  7903, 26068,  5883, 29943,\n",
      "        38815, 24848,  5891,  5783, 23318, 44025,  2160, 31013,   861, 44853,\n",
      "        31436, 35174, 36540, 35205,   654,  5372, 38621, 13844, 29274, 35456])\n",
      "In the world of artificial intelligence, language models have revolutionized how we process and generate text. \n",
      "These systems use architectures like transformers to learn complex patterns in textual data.\n",
      "The tokenization process converts text into numbers that the model can processethe\n",
      "Success! Output shape: torch.Size([1, 50, 50257])\n",
      "Sample logits: tensor([ 0.6738,  0.1613, -0.7650,  0.0471, -0.5036], grad_fn=<SliceBackward0>)\n",
      "\n",
      "Processing Batch 2:\n",
      "Input shape: torch.Size([1, 50])\n",
      "prompt_str  the world of artificial intelligence, language models have revolutionized how we process and generate text. \n",
      "These systems use architectures like transformers to learn complex patterns in textual data.\n",
      "The tokenization process converts text into numbers that the model can process efficiently\n",
      "in_idx.shape() torch.Size([1, 50])\n",
      "logits: tensor([[[ 0.9232,  0.6248, -0.5730,  ..., -0.5193, -0.0802,  0.9127],\n",
      "         [-0.2689,  0.9922,  0.2366,  ..., -0.3114, -0.6900,  0.6433],\n",
      "         [ 0.5444,  1.2447,  0.1101,  ...,  0.0221, -0.4531,  0.7618],\n",
      "         ...,\n",
      "         [-0.1528,  0.8201,  0.4415,  ..., -0.4049, -0.1680, -0.1634],\n",
      "         [ 0.3237,  0.4896,  0.1793,  ..., -0.1466, -0.3004, -0.2141],\n",
      "         [-0.6838, -0.4210, -0.4243,  ...,  0.5962, -0.6034,  0.4724]]],\n",
      "       grad_fn=<UnsafeViewBackward0>)\n",
      "torch.Size([1, 50, 50257])\n",
      "predicted_token_id tensor([ 8835,  3755, 25637, 25706, 40153, 43100, 17164, 27385,   984, 40575,\n",
      "         1371,  7523, 42269,  9091, 19631, 33728, 48075, 42421, 34500, 25829,\n",
      "        40299, 12083,  3681, 23041, 48045, 32258, 43699, 29041, 34494, 13583,\n",
      "        35188,  9599,  3361, 32359, 47753, 23199, 22305, 31943, 31539, 13429,\n",
      "         6087,  8189, 31696, 33673, 44160, 47579, 20759, 28662, 25391, 28713])\n",
      " the world of artificial intelligence, language models have revolutionized how we process and generate text. \n",
      "These systems use architectures like transformers to learn complex patterns in textual data.\n",
      "The tokenization process converts text into numbers that the model can process efficiently Hon\n",
      "Success! Output shape: torch.Size([1, 50, 50257])\n",
      "Sample logits: tensor([ 0.9232,  0.6248, -0.5730, -0.1962,  0.0192], grad_fn=<SliceBackward0>)\n",
      "\n",
      "Processing Batch 3:\n",
      "Input shape: torch.Size([1, 50])\n",
      "prompt_str  world of artificial intelligence, language models have revolutionized how we process and generate text. \n",
      "These systems use architectures like transformers to learn complex patterns in textual data.\n",
      "The tokenization process converts text into numbers that the model can process efficiently.\n",
      "in_idx.shape() torch.Size([1, 50])\n",
      "logits: tensor([[[ 0.0182,  1.4455, -0.1230,  ...,  0.7279, -0.6236,  0.2077],\n",
      "         [ 1.1693,  0.7229,  0.1207,  ..., -0.4815, -0.3517,  0.3452],\n",
      "         [-0.7221,  0.6602, -0.0146,  ..., -0.2292, -0.2322,  1.8905],\n",
      "         ...,\n",
      "         [-0.2598,  0.7625, -0.7193,  ...,  0.3327, -0.9569, -0.8930],\n",
      "         [-0.5690, -0.7470,  0.0275,  ...,  0.1164,  0.3712,  0.7852],\n",
      "         [-0.4690, -0.9804, -0.2155,  ...,  0.3645, -0.5793,  0.7344]]],\n",
      "       grad_fn=<UnsafeViewBackward0>)\n",
      "torch.Size([1, 50, 50257])\n",
      "predicted_token_id tensor([29478, 22181, 46164,  9169, 37314, 48779, 34024, 29174, 27857, 27279,\n",
      "        13547,  4647, 16845, 32563, 25800, 49617, 49133, 44752,  7703, 37574,\n",
      "        30562, 31045, 16933, 44646, 25371, 17906, 48877, 24985,  8745, 35565,\n",
      "        21503,  2378, 28798, 35271, 39212, 42559,  7805, 14559, 35426, 44853,\n",
      "        17433, 34851, 13325,  1145, 48090, 16359, 30076, 48059, 17006,  4268])\n",
      " world of artificial intelligence, language models have revolutionized how we process and generate text. \n",
      "These systems use architectures like transformers to learn complex patterns in textual data.\n",
      "The tokenization process converts text into numbers that the model can process efficiently.Hon\n",
      "Success! Output shape: torch.Size([1, 50, 50257])\n",
      "Sample logits: tensor([ 0.0182,  1.4455, -0.1230, -0.5424, -0.7443], grad_fn=<SliceBackward0>)\n",
      "\n",
      "Processing Batch 4:\n",
      "Input shape: torch.Size([1, 50])\n",
      "prompt_str  of artificial intelligence, language models have revolutionized how we process and generate text. \n",
      "These systems use architectures like transformers to learn complex patterns in textual data.\n",
      "The tokenization process converts text into numbers that the model can process efficiently.\n",
      "\n",
      "in_idx.shape() torch.Size([1, 50])\n",
      "logits: tensor([[[ 1.6362,  0.5896,  0.0698,  ..., -0.1630,  0.2954,  0.6013],\n",
      "         [ 0.0681,  0.1022, -0.3421,  ..., -0.4176, -0.2813,  1.2593],\n",
      "         [ 0.0684,  0.4218, -0.4751,  ...,  0.3033, -0.7760,  0.7014],\n",
      "         ...,\n",
      "         [-0.4952, -0.0223, -0.5567,  ...,  0.6068, -0.7507, -0.1837],\n",
      "         [-0.0972, -0.1829, -0.0341,  ..., -1.0945, -0.6798,  0.9897],\n",
      "         [-0.1520, -1.1838,  0.2695,  ...,  0.5331, -0.5490,  0.1968]]],\n",
      "       grad_fn=<UnsafeViewBackward0>)\n",
      "torch.Size([1, 50, 50257])\n",
      "predicted_token_id tensor([ 1496,  5275, 49727,   875, 48865, 27385,  5070, 30233, 41990, 36258,\n",
      "        40957, 14919, 12787, 12836, 19631,  9846, 19524, 40957, 19274, 20980,\n",
      "        29019, 40670, 27524, 48876, 11939, 36189, 43699, 20319, 28500,    47,\n",
      "        42087, 14001, 48749, 40666, 34491, 37477, 28221, 43146, 23465, 35321,\n",
      "         2709,  9772, 13676, 25939, 42051, 37949, 20889, 43847,  9436, 34535])\n",
      " of artificial intelligence, language models have revolutionized how we process and generate text. \n",
      "These systems use architectures like transformers to learn complex patterns in textual data.\n",
      "The tokenization process converts text into numbers that the model can process efficiently.\n",
      "pped\n",
      "Success! Output shape: torch.Size([1, 50, 50257])\n",
      "Sample logits: tensor([ 1.6362,  0.5896,  0.0698, -0.3440,  0.0275], grad_fn=<SliceBackward0>)\n",
      "\n",
      "Processing Batch 5:\n",
      "Input shape: torch.Size([1, 50])\n",
      "prompt_str  artificial intelligence, language models have revolutionized how we process and generate text. \n",
      "These systems use architectures like transformers to learn complex patterns in textual data.\n",
      "The tokenization process converts text into numbers that the model can process efficiently.\n",
      "Data\n",
      "in_idx.shape() torch.Size([1, 50])\n",
      "logits: tensor([[[-0.1285,  0.0394, -0.7255,  ..., -0.8578,  0.1264,  1.2684],\n",
      "         [-0.4327, -0.4481, -0.1903,  ...,  0.0386, -0.1578,  0.0878],\n",
      "         [-0.2828,  0.7949,  0.0264,  ..., -0.4288, -0.6050,  1.1229],\n",
      "         ...,\n",
      "         [-0.9577, -0.0374,  0.3018,  ..., -0.1375, -0.7912,  0.6802],\n",
      "         [ 0.4334, -0.7720,  0.4635,  ..., -0.2334, -0.0977, -0.1046],\n",
      "         [-0.6885, -0.2522,  0.8481,  ...,  1.4650, -0.5813, -0.0052]]],\n",
      "       grad_fn=<UnsafeViewBackward0>)\n",
      "torch.Size([1, 50, 50257])\n",
      "predicted_token_id tensor([ 8835, 25350, 49676, 44238, 14157,  4150, 21607,  8967, 42105, 16387,\n",
      "        40957,  4262, 47437, 39309, 46510, 36844, 22214,  2184, 20980,  6986,\n",
      "         6236, 14001, 27097,  9876, 28622, 48838,  5460, 15964, 21503,  7902,\n",
      "        44217,  8575, 12310,  7514,  5229, 39985,  4282, 32501, 26483, 20493,\n",
      "        10842, 24174, 19625, 18356, 33567,  7982, 22893, 47357, 26934, 18872])\n",
      " artificial intelligence, language models have revolutionized how we process and generate text. \n",
      "These systems use architectures like transformers to learn complex patterns in textual data.\n",
      "The tokenization process converts text into numbers that the model can process efficiently.\n",
      "Data Hon\n",
      "Success! Output shape: torch.Size([1, 50, 50257])\n",
      "Sample logits: tensor([-0.1285,  0.0394, -0.7255, -0.6751,  0.1715], grad_fn=<SliceBackward0>)\n",
      "\n",
      "Processing Batch 6:\n",
      "Input shape: torch.Size([1, 50])\n",
      "prompt_str  intelligence, language models have revolutionized how we process and generate text. \n",
      "These systems use architectures like transformers to learn complex patterns in textual data.\n",
      "The tokenization process converts text into numbers that the model can process efficiently.\n",
      "DataLoad\n",
      "in_idx.shape() torch.Size([1, 50])\n",
      "logits: tensor([[[ 0.6673,  0.3784, -0.5076,  ...,  0.3831,  0.3159,  0.1760],\n",
      "         [ 0.3624,  0.2452,  0.1467,  ...,  0.1379, -0.4659,  0.3514],\n",
      "         [-0.6567,  0.8278, -0.4077,  ...,  0.3016, -0.7119, -0.3438],\n",
      "         ...,\n",
      "         [ 0.1984, -0.3117, -0.1251,  ...,  0.1059, -0.6704, -0.5454],\n",
      "         [-0.1437, -0.0990,  0.7115,  ...,  0.3098,  0.1809,  0.3680],\n",
      "         [-0.1500, -0.1630,  0.3755,  ...,  1.7102,  0.1090,  0.2355]]],\n",
      "       grad_fn=<UnsafeViewBackward0>)\n",
      "torch.Size([1, 50, 50257])\n",
      "predicted_token_id tensor([23832,  4021, 35688,  6631, 24613, 16063, 10823, 17422, 15901, 48012,\n",
      "         4568,  7186,  9138,  8463,  1120, 22134,  8654, 42421, 29803,  5151,\n",
      "          608, 22573,  1978, 26299, 29215, 26316, 45489,  5891, 31184, 44626,\n",
      "        20939, 39212, 24048, 29563, 31809, 36337, 31400, 11155,  3388, 44508,\n",
      "        32464,  3201, 25785, 26124, 27161, 12376, 43893, 42818, 25115, 45318])\n",
      " intelligence, language models have revolutionized how we process and generate text. \n",
      "These systems use architectures like transformers to learn complex patterns in textual data.\n",
      "The tokenization process converts text into numbers that the model can process efficiently.\n",
      "DataLoad Prix\n",
      "Success! Output shape: torch.Size([1, 50, 50257])\n",
      "Sample logits: tensor([ 0.6673,  0.3784, -0.5076, -0.1659, -0.7421], grad_fn=<SliceBackward0>)\n",
      "\n",
      "Processing Batch 7:\n",
      "Input shape: torch.Size([1, 50])\n",
      "prompt_str , language models have revolutionized how we process and generate text. \n",
      "These systems use architectures like transformers to learn complex patterns in textual data.\n",
      "The tokenization process converts text into numbers that the model can process efficiently.\n",
      "DataLoaders\n",
      "in_idx.shape() torch.Size([1, 50])\n",
      "logits: tensor([[[ 1.0297, -0.1868, -0.3836,  ..., -0.5885,  0.0929,  1.2018],\n",
      "         [ 0.8092, -0.5889, -0.5838,  ..., -0.4473, -1.2303, -0.9179],\n",
      "         [-0.3030,  0.7960, -0.0564,  ...,  0.2989, -0.9321,  0.2102],\n",
      "         ...,\n",
      "         [-0.3162,  0.3150,  0.5424,  ...,  0.6371, -0.6519, -0.2033],\n",
      "         [-0.6198,  0.2509,  0.3861,  ...,  0.6873,  0.8158,  0.3771],\n",
      "         [-0.0863,  0.2619,  0.4299,  ...,  0.5898, -0.8550, -0.0445]]],\n",
      "       grad_fn=<UnsafeViewBackward0>)\n",
      "torch.Size([1, 50, 50257])\n",
      "predicted_token_id tensor([21385, 47846, 39917, 28575, 21655, 16308, 37265, 45723, 26525, 45531,\n",
      "         6328, 19042, 33000, 13698, 11221, 22633,  7241, 31093,  6266, 22199,\n",
      "         7001, 48417, 25416, 23041,   746, 14611,  3413, 18408, 18700,  5209,\n",
      "        21299, 28307,  4096, 13876, 28846, 19527, 10030, 33282,  9410, 42239,\n",
      "        31739,   265, 37102,  1521, 36259, 32097, 34428, 45373,  3922, 34400])\n",
      ", language models have revolutionized how we process and generate text. \n",
      "These systems use architectures like transformers to learn complex patterns in textual data.\n",
      "The tokenization process converts text into numbers that the model can process efficiently.\n",
      "DataLoaders bacon\n",
      "Success! Output shape: torch.Size([1, 50, 50257])\n",
      "Sample logits: tensor([ 1.0297, -0.1868, -0.3836,  0.3819, -0.3142], grad_fn=<SliceBackward0>)\n",
      "\n",
      "Processing Batch 8:\n",
      "Input shape: torch.Size([1, 50])\n",
      "prompt_str  language models have revolutionized how we process and generate text. \n",
      "These systems use architectures like transformers to learn complex patterns in textual data.\n",
      "The tokenization process converts text into numbers that the model can process efficiently.\n",
      "DataLoaders allow\n",
      "in_idx.shape() torch.Size([1, 50])\n",
      "logits: tensor([[[ 5.2664e-01,  3.0600e-01, -5.0138e-01,  ...,  3.0721e-01,\n",
      "          -5.7738e-01, -6.0640e-01],\n",
      "         [-1.6857e-01, -9.6682e-04, -1.2304e-01,  ..., -3.3317e-01,\n",
      "          -2.6550e-01, -6.2273e-01],\n",
      "         [-9.5578e-02,  6.8246e-01, -1.1777e-01,  ...,  1.1578e-01,\n",
      "          -1.3537e+00,  5.5578e-01],\n",
      "         ...,\n",
      "         [-6.8386e-01,  2.4791e-01, -6.6291e-02,  ...,  5.7695e-01,\n",
      "           1.5019e-01,  1.6700e-01],\n",
      "         [-2.1417e-01,  3.4197e-01,  4.7818e-01,  ..., -4.6018e-01,\n",
      "          -5.7949e-01, -7.5759e-02],\n",
      "         [-5.9462e-02,  1.2924e-01, -3.6536e-01,  ...,  4.4190e-01,\n",
      "           2.6690e-02, -8.5423e-02]]], grad_fn=<UnsafeViewBackward0>)\n",
      "torch.Size([1, 50, 50257])\n",
      "predicted_token_id tensor([17500, 38815, 42715, 39501, 19999, 15936, 50141, 37631, 20079,  9943,\n",
      "        26070,  1480, 24551, 34990, 44738,  7144,  6080, 23205, 11293, 48306,\n",
      "        10770, 22040, 22014,  3049, 33114, 18298,   102, 26064, 23284, 20318,\n",
      "         8230, 34284,   681,  6583, 29411, 41467,  5563, 10725, 23568, 30265,\n",
      "        28037, 47438, 10567, 20948, 29562, 16113, 44874, 11901, 49792, 43414])\n",
      " language models have revolutionized how we process and generate text. \n",
      "These systems use architectures like transformers to learn complex patterns in textual data.\n",
      "The tokenization process converts text into numbers that the model can process efficiently.\n",
      "DataLoaders allowosc\n",
      "Success! Output shape: torch.Size([1, 50, 50257])\n",
      "Sample logits: tensor([ 0.5266,  0.3060, -0.5014, -0.3649, -0.0110], grad_fn=<SliceBackward0>)\n",
      "\n",
      "Processing Batch 9:\n",
      "Input shape: torch.Size([1, 50])\n",
      "prompt_str  models have revolutionized how we process and generate text. \n",
      "These systems use architectures like transformers to learn complex patterns in textual data.\n",
      "The tokenization process converts text into numbers that the model can process efficiently.\n",
      "DataLoaders allow loading\n",
      "in_idx.shape() torch.Size([1, 50])\n",
      "logits: tensor([[[ 0.1758,  0.6586, -0.3452,  ..., -0.7787,  0.0543,  0.2794],\n",
      "         [-0.0273, -0.2368,  0.3969,  ..., -0.8008, -1.4564,  0.0992],\n",
      "         [-0.9578,  0.2871, -0.2084,  ...,  1.0004,  0.1896,  0.9967],\n",
      "         ...,\n",
      "         [-0.2970,  0.6661,  0.2958,  ..., -0.0870, -0.9051, -0.7316],\n",
      "         [ 0.1785, -0.0269,  0.1045,  ..., -0.8286,  0.7183,  0.3886],\n",
      "         [-0.4431, -0.2519, -0.4937,  ...,  1.0643, -0.2886,  0.7785]]],\n",
      "       grad_fn=<UnsafeViewBackward0>)\n",
      "torch.Size([1, 50, 50257])\n",
      "predicted_token_id tensor([ 5753, 26853, 49074, 46023, 32071, 49216,  1371,  5061, 35288, 13524,\n",
      "        19507, 14939, 14258,  2769, 39138, 27433, 47084, 24359, 11075,   566,\n",
      "         6319,  5777, 12587, 38474,  2381, 18298,   836, 20319,  8652,  5209,\n",
      "        31721, 29537, 21175, 29023, 30912,  8213, 33051, 37935,  2229, 37724,\n",
      "        46044, 30466,  9257, 26858, 22134, 42495,  3922,  5369, 32694,  4765])\n",
      " models have revolutionized how we process and generate text. \n",
      "These systems use architectures like transformers to learn complex patterns in textual data.\n",
      "The tokenization process converts text into numbers that the model can process efficiently.\n",
      "DataLoaders allow loadinguls\n",
      "Success! Output shape: torch.Size([1, 50, 50257])\n",
      "Sample logits: tensor([ 0.1758,  0.6586, -0.3452, -0.4632, -0.6929], grad_fn=<SliceBackward0>)\n",
      "\n",
      "Processing Batch 10:\n",
      "Input shape: torch.Size([1, 50])\n",
      "prompt_str  have revolutionized how we process and generate text. \n",
      "These systems use architectures like transformers to learn complex patterns in textual data.\n",
      "The tokenization process converts text into numbers that the model can process efficiently.\n",
      "DataLoaders allow loading and\n",
      "in_idx.shape() torch.Size([1, 50])\n",
      "logits: tensor([[[ 1.2054,  0.5833, -0.5568,  ..., -0.3678, -1.1347,  0.0584],\n",
      "         [ 0.1316, -0.8302,  0.0726,  ...,  0.2959,  0.3305,  0.2246],\n",
      "         [-0.3971,  0.7785, -0.5140,  ...,  0.0655, -0.8697,  0.9693],\n",
      "         ...,\n",
      "         [-0.1772,  0.5541, -0.5013,  ..., -0.4021,  0.0701, -0.1926],\n",
      "         [ 0.0619,  0.0701,  0.0802,  ...,  0.1854,  0.7706,  0.7900],\n",
      "         [-0.3760, -0.0275,  0.6401,  ...,  0.9158, -1.4878,  0.0417]]],\n",
      "       grad_fn=<UnsafeViewBackward0>)\n",
      "torch.Size([1, 50, 50257])\n",
      "predicted_token_id tensor([41301, 41325, 25637,  8757, 42269,  4041,  8911, 30835, 11802, 25544,\n",
      "          299, 31752, 23430, 11722, 32219, 10096, 37781,  6314, 13590, 32869,\n",
      "        10770, 33816,  3681, 33431,  5108, 18298,  8575, 31314, 23284, 37477,\n",
      "        27699,  1032, 32794, 21259, 11253, 47132,  2146, 39915, 12749, 40939,\n",
      "        19449, 12304, 13325,  3683, 23860,  3922, 32366, 26473, 29029,  4568])\n",
      " have revolutionized how we process and generate text. \n",
      "These systems use architectures like transformers to learn complex patterns in textual data.\n",
      "The tokenization process converts text into numbers that the model can process efficiently.\n",
      "DataLoaders allow loading andrified\n",
      "Success! Output shape: torch.Size([1, 50, 50257])\n",
      "Sample logits: tensor([ 1.2054,  0.5833, -0.5568, -0.3705, -0.1867], grad_fn=<SliceBackward0>)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "sys.path.append('./lib')\n",
    "\n",
    "from dataloader import create_dataloader\n",
    "from gpt_model import GPTModel\n",
    "import torch\n",
    "import tiktoken\n",
    "tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
    "\n",
    "print(\"Starting GPT test...\")\n",
    "\n",
    "# Test text\n",
    "sample_text = \"\"\"\n",
    "In the world of artificial intelligence, language models have revolutionized how we process and generate text. \n",
    "These systems use architectures like transformers to learn complex patterns in textual data.\n",
    "The tokenization process converts text into numbers that the model can process efficiently.\n",
    "DataLoaders allow loading and processing data in batches during training.\n",
    "The sliding window technique with stride helps create overlapping sequences for better learning.\n",
    "\"\"\"\n",
    "\n",
    "GPT_CONFIG_124M = {\n",
    "    \"vocab_size\": 50257,   # Vocabulary size\n",
    "    \"context_length\": 256, # Shortened context length (orig: 1024)\n",
    "    \"emb_dim\": 768,        # Embedding dimension\n",
    "    \"n_heads\": 12,         # Number of attention heads\n",
    "    \"n_layers\": 12,        # Number of layers\n",
    "    \"drop_rate\": 0.1,      # Dropout rate\n",
    "    \"qkv_bias\": False      # Query-key-value bias\n",
    "}\n",
    "\n",
    "print(\"Creating GPT model...\")\n",
    "model = GPTModel(GPT_CONFIG_124M)\n",
    "print(\"Model created successfully!\")\n",
    "\n",
    "print(\"Creating dataloader...\")\n",
    "dataloader = create_dataloader(\n",
    "    text=sample_text,\n",
    "    batch_size=1,\n",
    "    max_length=50,  \n",
    "    stride=1,      \n",
    "    shuffle=False \n",
    ")\n",
    "print(f\"Dataloader created! Dataset size: {len(dataloader.dataset)}\")\n",
    "print(f\"Number of batches: {len(dataloader)}\")\n",
    "\n",
    "print(\"Starting forward pass...\")\n",
    "for i, (inputs, targets) in enumerate(dataloader):\n",
    "    if i >= 11:  # Only process first batch\n",
    "        break\n",
    "    print(f\"Processing Batch {i}:\")\n",
    "    print(f\"Input shape: {inputs.shape}\")\n",
    "    \n",
    "    try:\n",
    "        a = inputs[0].tolist()\n",
    "        prompt_str = tokenizer.decode(a)\n",
    "        print(\"prompt_str\", prompt_str)\n",
    "        logits = model.forward(inputs)\n",
    "        print(\"logits:\", logits)\n",
    "        print(logits.shape)\n",
    "        predicted_token_id = torch.argmax(logits, dim=-1)[-1]\n",
    "        print(\"predicted_token_id\", predicted_token_id)\n",
    "        a.append(predicted_token_id[0])\n",
    "        b = tokenizer.decode(a)\n",
    "        print(b)\n",
    "        print(f\"Success! Output shape: {logits.shape}\")\n",
    "        print(f\"Sample logits: {logits[0, 0, :5]}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error during forward pass: {e}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f30d3193",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "3.12.11",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
