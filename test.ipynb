{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "78558224",
   "metadata": {},
   "source": [
    "# DataLoader Tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a8cc03ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/peter/.pyenv/versions/3.12.11/lib/python3.12/site-packages/torch/_subclasses/functional_tensor.py:279: UserWarning: Failed to initialize NumPy: No module named 'numpy' (Triggered internally at /pytorch/torch/csrc/utils/tensor_numpy.cpp:81.)\n",
      "  cpu = _conversion_method_template(device=torch.device(\"cpu\"))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Testing DataLoader with different stride values ===\n",
      "\n",
      "--- STRIDE = 5 ---\n",
      "Dataset size: 13 sequences\n",
      "Number of batches: 6\n",
      "Batch 0:\n",
      "  Input shape: torch.Size([2, 20])\n",
      "  Input tokens (first seq): [198, 818, 262, 995, 286, 11666, 4430, 11, 3303, 4981, 423, 5854, 1143, 703, 356, 1429, 290, 7716, 2420, 13]...\n",
      "  Target tokens (first seq): [818, 262, 995, 286, 11666, 4430, 11, 3303, 4981, 423, 5854, 1143, 703, 356, 1429, 290, 7716, 2420, 13, 220]...\n",
      "  Target text (first seq): In the world of artificial intelligence, language models have revolutionized how we process and generate text. \n",
      "  Input tokens (second seq): [11666, 4430, 11, 3303, 4981, 423, 5854, 1143, 703, 356]...\n",
      "  Target tokens (second seq): [4430, 11, 3303, 4981, 423, 5854, 1143, 703, 356, 1429]...\n",
      "\n",
      "Batch 1:\n",
      "  Input shape: torch.Size([2, 20])\n",
      "  Input tokens (first seq): [423, 5854, 1143, 703, 356, 1429, 290, 7716, 2420, 13, 220, 198, 4711, 3341, 779, 45619, 588, 6121, 364, 284]...\n",
      "  Target tokens (first seq): [5854, 1143, 703, 356, 1429, 290, 7716, 2420, 13, 220, 198, 4711, 3341, 779, 45619, 588, 6121, 364, 284, 2193]...\n",
      "  Target text (first seq):  revolutionized how we process and generate text. \n",
      "These systems use architectures like transformers to learn\n",
      "  Input tokens (second seq): [1429, 290, 7716, 2420, 13, 220, 198, 4711, 3341, 779]...\n",
      "  Target tokens (second seq): [290, 7716, 2420, 13, 220, 198, 4711, 3341, 779, 45619]...\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "sys.path.append('./lib')\n",
    "\n",
    "from dataloader import create_dataloader\n",
    "import torch\n",
    "import tiktoken\n",
    "tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
    "\n",
    "# Test text\n",
    "sample_text = \"\"\"\n",
    "In the world of artificial intelligence, language models have revolutionized how we process and generate text. \n",
    "These systems use architectures like transformers to learn complex patterns in textual data.\n",
    "The tokenization process converts text into numbers that the model can process efficiently.\n",
    "DataLoaders allow loading and processing data in batches during training.\n",
    "The sliding window technique with stride helps create overlapping sequences for better learning.\n",
    "\"\"\"\n",
    "\n",
    "print(\"=== Testing DataLoader with different stride values ===\\n\")\n",
    "\n",
    "# Base configuration\n",
    "batch_size = 2\n",
    "max_length = 20\n",
    "\n",
    "# Test with different stride values\n",
    "strides = [5]\n",
    "\n",
    "for stride in strides:\n",
    "    print(f\"--- STRIDE = {stride} ---\")\n",
    "    \n",
    "    # Create DataLoader\n",
    "    dataloader = create_dataloader(\n",
    "        text=sample_text,\n",
    "        batch_size=batch_size,\n",
    "        max_length=max_length,\n",
    "        stride=stride,\n",
    "        shuffle=False \n",
    "    )\n",
    "    \n",
    "    print(f\"Dataset size: {len(dataloader.dataset)} sequences\")\n",
    "\n",
    "    print(f\"Number of batches: {len(dataloader)}\")\n",
    "    \n",
    "    # Show first 2 batches\n",
    "    for i, (inputs, targets) in enumerate(dataloader):\n",
    "        if i >= 2:  # Only show first 2 batches\n",
    "            break\n",
    "        print(f\"Batch {i}:\")\n",
    "        print(f\"  Input shape: {inputs.shape}\")\n",
    "        print(f\"  Input tokens (first seq): {inputs[0][:30].tolist()}...\")\n",
    "        print(f\"  Target tokens (first seq): {targets[0][:30].tolist()}...\")\n",
    "        print(f\"  Target text (first seq): {tokenizer.decode(targets[0].tolist())}\")\n",
    "        print(f\"  Input tokens (second seq): {inputs[1][:10].tolist()}...\")\n",
    "        print(f\"  Target tokens (second seq): {targets[1][:10].tolist()}...\")\n",
    "        print()\n",
    "    \n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e4d1917c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Batch 0 ---\n",
      "Batch size: 2\n",
      "Sequence length: 20\n",
      "Token embeddings shape: torch.Size([2, 20, 768])\n",
      "tensor([[[ 0.9310, -2.3801,  0.4624,  ..., -1.0475, -0.0131, -0.9843],\n",
      "         [ 0.6364,  0.2973, -0.9704,  ..., -0.9287, -1.3964,  0.5278],\n",
      "         [-0.9624,  1.2035,  0.5642,  ..., -0.6229,  1.1756, -0.6710],\n",
      "         ...,\n",
      "         [-0.4829,  1.0758,  0.9326,  ..., -0.6283,  0.9120, -2.0170],\n",
      "         [-0.0718, -0.3283, -0.2922,  ...,  0.1341,  0.0188, -0.3485],\n",
      "         [-1.7194,  0.7332, -0.5110,  ...,  1.1052,  0.6247,  0.8298]],\n",
      "\n",
      "        [[-1.8054,  1.5085,  1.5272,  ..., -1.2398, -0.4727, -0.2368],\n",
      "         [-0.0681,  0.1716,  0.6419,  ...,  0.8225, -0.7575, -0.0843],\n",
      "         [ 0.4913, -1.4582, -0.8070,  ...,  0.2394,  3.2997, -0.0060],\n",
      "         ...,\n",
      "         [ 1.7950, -0.2159, -0.6956,  ...,  0.6743,  0.3231,  0.6592],\n",
      "         [ 1.6259, -1.7140, -0.0779,  ..., -0.3911,  0.0937,  0.5138],\n",
      "         [-1.1236,  0.0723, -0.4045,  ..., -0.3030,  0.8140, -1.7924]]],\n",
      "       grad_fn=<EmbeddingBackward0>)\n",
      "Position indices: tensor([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17,\n",
      "        18, 19])\n",
      "Position embeddings shape: torch.Size([20, 768])\n",
      "tensor([[-0.1898,  0.8335,  0.8752,  ..., -0.4366, -1.4388,  0.7477],\n",
      "        [ 1.5234, -1.0255,  0.6657,  ..., -1.6874, -0.2827,  0.0034],\n",
      "        [-1.6359,  0.2523,  1.5682,  ...,  0.7656,  1.6832, -0.7896],\n",
      "        ...,\n",
      "        [ 1.2062, -0.0821,  0.6119,  ...,  0.1085, -0.7126,  0.9570],\n",
      "        [ 0.9665,  0.0950,  0.3335,  ...,  0.9564,  0.6216,  0.6789],\n",
      "        [-1.4211,  0.5299,  1.1318,  ..., -0.3394,  0.1263, -1.5727]],\n",
      "       grad_fn=<EmbeddingBackward0>)\n",
      "Final result shape: torch.Size([2, 20, 768])\n",
      "tensor([[[ 0.8235, -1.7184,  1.4862,  ..., -1.6489, -1.6132, -0.2629],\n",
      "         [ 2.3998, -0.8091, -0.3385,  ..., -2.9068, -1.8657,  0.5902],\n",
      "         [-2.8870,  1.6176,  2.3693,  ...,  0.1586,  0.0000, -1.6229],\n",
      "         ...,\n",
      "         [ 0.8037,  1.1041,  1.7162,  ..., -0.5776,  0.2216, -1.1779],\n",
      "         [ 0.9942, -0.2592,  0.0459,  ...,  1.2118,  0.7115,  0.3671],\n",
      "         [-3.4894,  1.4034,  0.6898,  ...,  0.8508,  0.8344, -0.0000]],\n",
      "\n",
      "        [[-2.2169,  2.6022,  2.6694,  ..., -1.8626, -2.1239,  0.5677],\n",
      "         [ 1.6170, -0.9488,  1.4530,  ..., -0.9611, -1.1558, -0.0899],\n",
      "         [-1.2717, -1.3399,  0.8458,  ...,  1.1167,  0.0000, -0.8840],\n",
      "         ...,\n",
      "         [ 3.3346, -0.0000, -0.0000,  ...,  0.8698, -0.0000,  1.7958],\n",
      "         [ 2.8805, -1.7988,  0.2839,  ...,  0.6281,  0.0000,  1.3252],\n",
      "         [-2.8275,  0.6691,  0.8082,  ..., -0.7138,  1.0447, -3.7390]]],\n",
      "       grad_fn=<MulBackward0>)\n",
      "\n",
      "--- Batch 1 ---\n",
      "Batch size: 2\n",
      "Sequence length: 20\n",
      "Token embeddings shape: torch.Size([2, 20, 768])\n",
      "tensor([[[-0.2560,  0.4674,  1.2532,  ...,  0.1973,  1.7720,  1.1970],\n",
      "         [ 1.5500,  0.9145, -1.4108,  ..., -1.6414,  0.4252,  1.0439],\n",
      "         [-0.1276, -2.4309, -0.6045,  ...,  0.4012,  1.4511,  0.3554],\n",
      "         ...,\n",
      "         [-0.8246, -1.0695,  0.1502,  ..., -0.2776,  0.3911, -0.6939],\n",
      "         [ 0.7454,  0.0078, -0.5588,  ...,  0.6790,  0.5878,  0.3175],\n",
      "         [-0.8626, -0.1472,  0.0463,  ..., -1.4832,  0.2503, -0.0858]],\n",
      "\n",
      "        [[ 3.0361,  0.9879,  0.5743,  ..., -0.6133, -0.0758,  0.2454],\n",
      "         [-0.0197, -1.0335,  0.5742,  ..., -0.5746, -0.1313, -0.3461],\n",
      "         [-0.4829,  1.0758,  0.9326,  ..., -0.6283,  0.9120, -2.0170],\n",
      "         ...,\n",
      "         [-0.5981, -0.2266,  1.1639,  ..., -0.3421,  2.3325,  0.0249],\n",
      "         [-0.0218, -1.4182,  0.2741,  ...,  0.0417,  0.2016,  0.9078],\n",
      "         [ 1.0084,  0.3784, -1.8286,  ...,  0.2326, -2.3891,  0.9467]]],\n",
      "       grad_fn=<EmbeddingBackward0>)\n",
      "Position indices: tensor([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17,\n",
      "        18, 19])\n",
      "Position embeddings shape: torch.Size([20, 768])\n",
      "tensor([[-0.1898,  0.8335,  0.8752,  ..., -0.4366, -1.4388,  0.7477],\n",
      "        [ 1.5234, -1.0255,  0.6657,  ..., -1.6874, -0.2827,  0.0034],\n",
      "        [-1.6359,  0.2523,  1.5682,  ...,  0.7656,  1.6832, -0.7896],\n",
      "        ...,\n",
      "        [ 1.2062, -0.0821,  0.6119,  ...,  0.1085, -0.7126,  0.9570],\n",
      "        [ 0.9665,  0.0950,  0.3335,  ...,  0.9564,  0.6216,  0.6789],\n",
      "        [-1.4211,  0.5299,  1.1318,  ..., -0.3394,  0.1263, -1.5727]],\n",
      "       grad_fn=<EmbeddingBackward0>)\n",
      "Final result shape: torch.Size([2, 20, 768])\n",
      "tensor([[[-0.4954,  1.4455,  2.3649,  ..., -0.2658,  0.3701,  2.1608],\n",
      "         [ 3.4149, -0.1233, -0.8279,  ..., -3.6987,  0.1583,  1.1637],\n",
      "         [-1.9594, -2.4206,  0.0000,  ...,  1.2965,  3.4825, -0.4824],\n",
      "         ...,\n",
      "         [ 0.4240, -1.2796,  0.8468,  ..., -0.0000, -0.3572,  0.2923],\n",
      "         [ 1.9021,  0.0000, -0.0000,  ...,  1.8172,  1.3437,  1.1072],\n",
      "         [-2.5374,  0.4253,  1.3091,  ..., -2.0251,  0.4184, -0.0000]],\n",
      "\n",
      "        [[ 3.1625,  2.0239,  0.0000,  ..., -1.1665, -1.6829,  1.1034],\n",
      "         [ 1.6708, -0.0000,  1.3777,  ..., -2.5134, -0.4601, -0.3808],\n",
      "         [-2.3542,  1.4757,  2.7787,  ...,  0.1525,  2.8835, -0.0000],\n",
      "         ...,\n",
      "         [ 0.6756, -0.3431,  1.9732,  ..., -0.2595,  0.0000,  1.0910],\n",
      "         [ 1.0497, -1.4702,  0.6751,  ...,  1.1091,  0.9146,  1.7630],\n",
      "         [-0.4585,  1.0093, -0.7742,  ..., -0.1186, -2.5143, -0.6956]]],\n",
      "       grad_fn=<MulBackward0>)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "GPT_CONFIG_124M = {\n",
    "    \"vocab_size\": 50257,   # Vocabulary size\n",
    "    \"context_length\": 256, # Shortened context length (orig: 1024)\n",
    "    \"emb_dim\": 768,        # Embedding dimension\n",
    "    \"n_heads\": 12,         # Number of attention heads\n",
    "    \"n_layers\": 12,        # Number of layers\n",
    "    \"drop_rate\": 0.1,      # Dropout rate\n",
    "    \"qkv_bias\": False      # Query-key-value bias\n",
    "}\n",
    "\n",
    "\n",
    "tok_emb = torch.nn.Embedding(GPT_CONFIG_124M[\"vocab_size\"], GPT_CONFIG_124M[\"emb_dim\"])\n",
    "pos_emb = torch.nn.Embedding(GPT_CONFIG_124M[\"context_length\"], GPT_CONFIG_124M[\"emb_dim\"])\n",
    "drop_emb = torch.nn.Dropout(GPT_CONFIG_124M[\"drop_rate\"])\n",
    "\n",
    "\n",
    "for i, (inputs, targets) in enumerate(dataloader):\n",
    "    if i >= 2:  \n",
    "        break\n",
    "\n",
    "    batch_size, seq_len = inputs.shape\n",
    "    print(f\"--- Batch {i} ---\")\n",
    "    print(f\"Batch size: {batch_size}\")\n",
    "    print(f\"Sequence length: {seq_len}\")\n",
    "\n",
    "    tok_embeds = tok_emb(inputs)\n",
    "    print(f\"Token embeddings shape: {tok_embeds.shape}\")\n",
    "    print(tok_embeds)\n",
    "\n",
    "    pos_indices = torch.arange(seq_len)\n",
    "    print(f\"Position indices: {pos_indices}\")\n",
    "    pos_embeds = pos_emb(pos_indices)\n",
    "    print(f\"Position embeddings shape: {pos_embeds.shape}\")\n",
    "    print(pos_embeds)\n",
    "\n",
    "    final_embeds = tok_embeds + pos_embeds\n",
    "    result = drop_emb(final_embeds)\n",
    "    print(f\"Final result shape: {result.shape}\")\n",
    "    print(result)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "2f3a6150",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, d_in, d_out, context_length, dropout, num_heads, qkv_bias=False):\n",
    "        super().__init__()\n",
    "        assert d_out % num_heads == 0, \"d_out must be divisible by n_heads\"\n",
    "\n",
    "        self.d_out = d_out\n",
    "        self.num_heads = num_heads\n",
    "        self.head_dim = d_out // num_heads  # Reduce the projection dim to match desired output dim\n",
    "\n",
    "        self.W_query = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "        self.W_key = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "        self.W_value = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "        self.out_proj = nn.Linear(d_out, d_out)  # Linear layer to combine head outputs\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.register_buffer('mask', torch.triu(torch.ones(context_length, context_length), diagonal=1))\n",
    "\n",
    "    def forward(self, x):\n",
    "        print(\"=\" * 50)\n",
    "        print(\"MULTIHEAD ATTENTION FORWARD PASS\")\n",
    "        print(\"=\" * 50)\n",
    "        print(\"W_key weight matrix:\")\n",
    "        print(self.W_key.weight)\n",
    "        \n",
    "        b, num_tokens, d_in = x.shape\n",
    "        print(f\"INPUT: batch_size={b}, num_tokens={num_tokens}, d_in={d_in}\")\n",
    "        print(f\"Input shape: {x.shape}\")\n",
    "        print(f\"Configuration: d_out={self.d_out}, num_heads={self.num_heads}, head_dim={self.head_dim}\")\n",
    "        print()\n",
    "\n",
    "        # Linear transformations\n",
    "        print(\"1. LINEAR TRANSFORMATIONS (Q, K, V)\")\n",
    "        print(\"-\" * 30)\n",
    "        print(\"W_key weight matrix:\")\n",
    "        print(self.W_key.weight)\n",
    "        keys = self.W_key(x)  # Shape: (b, num_tokens, d_out)\n",
    "        queries = self.W_query(x)\n",
    "        values = self.W_value(x)\n",
    "        print(f\"After linear layers:\")\n",
    "        print(f\"  Keys shape: {keys.shape}\")\n",
    "        print(f\"  Keys tensor:\")\n",
    "        print(keys)\n",
    "        print(f\"  Queries shape: {queries.shape}\")\n",
    "        print(f\"  Values shape: {values.shape}\")\n",
    "        print()\n",
    "\n",
    "        # We implicitly split the matrix by adding a `num_heads` dimension\n",
    "        # Unroll last dim: (b, num_tokens, d_out) -> (b, num_tokens, num_heads, head_dim)\n",
    "        print(\"2. VIEW OPERATION - SPLIT INTO HEADS\")\n",
    "        print(\"-\" * 30)\n",
    "        print(f\"Reshaping from (b, num_tokens, d_out) to (b, num_tokens, num_heads, head_dim)\")\n",
    "        print(f\"  d_out={self.d_out} = num_heads={self.num_heads} × head_dim={self.head_dim}\")\n",
    "        \n",
    "        keys = keys.view(b, num_tokens, self.num_heads, self.head_dim)\n",
    "        values = values.view(b, num_tokens, self.num_heads, self.head_dim)\n",
    "        queries = queries.view(b, num_tokens, self.num_heads, self.head_dim)\n",
    "        \n",
    "        print(f\"After view operation:\")\n",
    "        print(f\"  Keys shape: {keys.shape}\")\n",
    "        print(f\"  Keys AW: {keys}\")\n",
    "        print(f\"  Queries shape: {queries.shape}\")\n",
    "        print(f\"  Values shape: {values.shape}\")\n",
    "        print()\n",
    "\n",
    "        # Transpose: (b, num_tokens, num_heads, head_dim) -> (b, num_heads, num_tokens, head_dim)\n",
    "        print(\"3. TRANSPOSE OPERATION\")\n",
    "        print(\"-\" * 30)\n",
    "        print(f\"Transposing dimensions 1 and 2:\")\n",
    "        print(f\"  (b, num_tokens, num_heads, head_dim) -> (b, num_heads, num_tokens, head_dim)\")\n",
    "        \n",
    "        keys = keys.transpose(1, 2)\n",
    "        queries = queries.transpose(1, 2)\n",
    "        values = values.transpose(1, 2)\n",
    "        \n",
    "        print(f\"After transpose:\")\n",
    "        print(f\"  Keys shape: {keys.shape}\")\n",
    "        print(f\"  Keys AT: {keys}\")\n",
    "        print(f\"  Queries shape: {queries.shape}\")\n",
    "        print(f\"  Values shape: {values.shape}\")\n",
    "        print()\n",
    "\n",
    "        # Compute scaled dot-product attention (aka self-attention) with a causal mask\n",
    "        print(\"4. ATTENTION SCORES COMPUTATION\")\n",
    "        print(\"-\" * 30)\n",
    "        print(f\"Computing Q @ K^T for each head:\")\n",
    "        print(f\"  queries shape: {queries.shape}\")\n",
    "        print(f\"  keys.transpose(2,3) shape: {keys.transpose(2, 3).shape}\")\n",
    "        \n",
    "        attn_scores = queries @ keys.transpose(2, 3)  # Dot product for each head\n",
    "        print(f\"Attention scores shape: {attn_scores.shape}\")\n",
    "        print(f\"Scaling factor (sqrt(head_dim)): {keys.shape[-1]**0.5}\")\n",
    "        print()\n",
    "\n",
    "        # Original mask truncated to the number of tokens and converted to boolean\n",
    "        print(\"5. CAUSAL MASK APPLICATION\")\n",
    "        print(\"-\" * 30)\n",
    "        mask_bool = self.mask.bool()[:num_tokens, :num_tokens]\n",
    "        print(f\"Mask shape: {mask_bool.shape}\")\n",
    "        print(f\"Mask (True=masked, False=allowed):\")\n",
    "        print(mask_bool)\n",
    "        print()\n",
    "\n",
    "        # Use the mask to fill attention scores\n",
    "        print(\"Before masking - attention scores (first head of first batch):\")\n",
    "        print(attn_scores[0, 0])\n",
    "        attn_scores.masked_fill_(mask_bool, -torch.inf)\n",
    "        print(\"After masking - attention scores (first head of first batch):\")\n",
    "        print(attn_scores[0, 0])\n",
    "        print()\n",
    "\n",
    "        print(\"6. SOFTMAX AND DROPOUT\")\n",
    "        print(\"-\" * 30)\n",
    "        attn_weights = torch.softmax(attn_scores / keys.shape[-1]**0.5, dim=-1)\n",
    "        print(f\"Attention weights shape: {attn_weights.shape}\")\n",
    "        print(\"Attention weights (first head of first batch):\")\n",
    "        print(attn_weights[0, 0])\n",
    "        \n",
    "        attn_weights = self.dropout(attn_weights)\n",
    "        print(\"After dropout applied\")\n",
    "        print()\n",
    "\n",
    "        # Shape: (b, num_tokens, num_heads, head_dim)\n",
    "        print(\"7. WEIGHTED VALUES COMPUTATION\")\n",
    "        print(\"-\" * 30)\n",
    "        print(f\"Computing attention_weights @ values:\")\n",
    "        print(f\"  attention_weights shape: {attn_weights.shape}\")\n",
    "        print(f\"  values shape: {values.shape}\")\n",
    "        \n",
    "        context_vec = (attn_weights @ values).transpose(1, 2)\n",
    "        print(f\"Context vector after transpose: {context_vec.shape}\")\n",
    "        print()\n",
    "\n",
    "        # Combine heads, where self.d_out = self.num_heads * self.head_dim\n",
    "        print(\"8. COMBINE HEADS (RESHAPE)\")\n",
    "        print(\"-\" * 30)\n",
    "        print(f\"Reshaping from {context_vec.shape} to (b, num_tokens, d_out)\")\n",
    "        context_vec = context_vec.reshape(b, num_tokens, self.d_out)\n",
    "        print(f\"Combined heads shape: {context_vec.shape}\")\n",
    "        \n",
    "        print(\"9. OUTPUT PROJECTION\")\n",
    "        print(\"-\" * 30)\n",
    "        context_vec = self.out_proj(context_vec)  # optional projection\n",
    "        print(f\"Final output shape: {context_vec.shape}\")\n",
    "        print()\n",
    "\n",
    "        return context_vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "9c3511a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input shape: torch.Size([2, 4, 6])\n",
      "Input tensor:\n",
      "tensor([[[ 1.0000,  2.0000,  3.0000,  4.0000,  5.0000,  6.0000],\n",
      "         [ 0.5000,  1.5000,  2.5000,  3.5000,  4.5000,  5.5000],\n",
      "         [ 2.0000,  4.0000,  6.0000,  8.0000, 10.0000, 12.0000],\n",
      "         [ 1.0000,  0.0000, -1.0000,  2.0000,  3.0000, -2.0000]],\n",
      "\n",
      "        [[ 0.1000,  0.2000,  0.3000,  0.4000,  0.5000,  0.6000],\n",
      "         [ 1.1000,  1.2000,  1.3000,  1.4000,  1.5000,  1.6000],\n",
      "         [-1.0000,  2.0000, -3.0000,  4.0000, -5.0000,  6.0000],\n",
      "         [ 0.0000,  1.0000,  0.0000,  1.0000,  0.0000,  1.0000]]])\n",
      "\n",
      "==================================================\n",
      "MULTIHEAD ATTENTION FORWARD PASS\n",
      "==================================================\n",
      "W_key weight matrix:\n",
      "Parameter containing:\n",
      "tensor([[-0.2847, -0.4029, -0.3316,  0.3044,  0.1960,  0.3435],\n",
      "        [ 0.2139,  0.1033, -0.0040, -0.3105, -0.3498, -0.3819],\n",
      "        [ 0.1671, -0.2004, -0.0822, -0.2349, -0.0744, -0.2873],\n",
      "        [-0.2668,  0.1354, -0.1213,  0.2520, -0.1310, -0.2995],\n",
      "        [-0.0720, -0.1979, -0.1249, -0.3887,  0.2284, -0.2842],\n",
      "        [ 0.2052,  0.1853,  0.2917, -0.3131,  0.2936, -0.1930]],\n",
      "       requires_grad=True)\n",
      "INPUT: batch_size=2, num_tokens=4, d_in=6\n",
      "Input shape: torch.Size([2, 4, 6])\n",
      "Configuration: d_out=6, num_heads=3, head_dim=2\n",
      "\n",
      "1. LINEAR TRANSFORMATIONS (Q, K, V)\n",
      "------------------------------\n",
      "W_key weight matrix:\n",
      "Parameter containing:\n",
      "tensor([[-0.2847, -0.4029, -0.3316,  0.3044,  0.1960,  0.3435],\n",
      "        [ 0.2139,  0.1033, -0.0040, -0.3105, -0.3498, -0.3819],\n",
      "        [ 0.1671, -0.2004, -0.0822, -0.2349, -0.0744, -0.2873],\n",
      "        [-0.2668,  0.1354, -0.1213,  0.2520, -0.1310, -0.2995],\n",
      "        [-0.0720, -0.1979, -0.1249, -0.3887,  0.2284, -0.2842],\n",
      "        [ 0.2052,  0.1853,  0.2917, -0.3131,  0.2936, -0.1930]],\n",
      "       requires_grad=True)\n",
      "After linear layers:\n",
      "  Keys shape: torch.Size([2, 4, 6])\n",
      "  Keys tensor:\n",
      "tensor([[[ 2.1738, -4.8734, -3.5160, -1.8035, -2.9604,  0.5082],\n",
      "         [ 2.2614, -4.5089, -3.1600, -1.5880, -2.5408,  0.2734],\n",
      "         [ 4.3476, -9.7468, -7.0321, -3.6070, -5.9208,  1.0163],\n",
      "         [ 0.5567, -0.6887,  0.1309,  0.5647,  0.5293,  0.5541]],\n",
      "\n",
      "        [[ 0.2174, -0.4873, -0.3516, -0.1804, -0.2960,  0.0508],\n",
      "         [ 0.0422, -1.2162, -1.0638, -0.6114, -1.1353,  0.5204],\n",
      "         [ 2.7725, -1.7794, -2.6134,  0.7677, -4.3511, -4.5883],\n",
      "         [ 0.2451, -0.5890, -0.7227,  0.0880, -0.8708, -0.3209]]],\n",
      "       grad_fn=<UnsafeViewBackward0>)\n",
      "  Queries shape: torch.Size([2, 4, 6])\n",
      "  Values shape: torch.Size([2, 4, 6])\n",
      "\n",
      "2. VIEW OPERATION - SPLIT INTO HEADS\n",
      "------------------------------\n",
      "Reshaping from (b, num_tokens, d_out) to (b, num_tokens, num_heads, head_dim)\n",
      "  d_out=6 = num_heads=3 × head_dim=2\n",
      "After view operation:\n",
      "  Keys shape: torch.Size([2, 4, 3, 2])\n",
      "  Keys AW: tensor([[[[ 2.1738, -4.8734],\n",
      "          [-3.5160, -1.8035],\n",
      "          [-2.9604,  0.5082]],\n",
      "\n",
      "         [[ 2.2614, -4.5089],\n",
      "          [-3.1600, -1.5880],\n",
      "          [-2.5408,  0.2734]],\n",
      "\n",
      "         [[ 4.3476, -9.7468],\n",
      "          [-7.0321, -3.6070],\n",
      "          [-5.9208,  1.0163]],\n",
      "\n",
      "         [[ 0.5567, -0.6887],\n",
      "          [ 0.1309,  0.5647],\n",
      "          [ 0.5293,  0.5541]]],\n",
      "\n",
      "\n",
      "        [[[ 0.2174, -0.4873],\n",
      "          [-0.3516, -0.1804],\n",
      "          [-0.2960,  0.0508]],\n",
      "\n",
      "         [[ 0.0422, -1.2162],\n",
      "          [-1.0638, -0.6114],\n",
      "          [-1.1353,  0.5204]],\n",
      "\n",
      "         [[ 2.7725, -1.7794],\n",
      "          [-2.6134,  0.7677],\n",
      "          [-4.3511, -4.5883]],\n",
      "\n",
      "         [[ 0.2451, -0.5890],\n",
      "          [-0.7227,  0.0880],\n",
      "          [-0.8708, -0.3209]]]], grad_fn=<ViewBackward0>)\n",
      "  Queries shape: torch.Size([2, 4, 3, 2])\n",
      "  Values shape: torch.Size([2, 4, 3, 2])\n",
      "\n",
      "3. TRANSPOSE OPERATION\n",
      "------------------------------\n",
      "Transposing dimensions 1 and 2:\n",
      "  (b, num_tokens, num_heads, head_dim) -> (b, num_heads, num_tokens, head_dim)\n",
      "After transpose:\n",
      "  Keys shape: torch.Size([2, 3, 4, 2])\n",
      "  Keys AT: tensor([[[[ 2.1738, -4.8734],\n",
      "          [ 2.2614, -4.5089],\n",
      "          [ 4.3476, -9.7468],\n",
      "          [ 0.5567, -0.6887]],\n",
      "\n",
      "         [[-3.5160, -1.8035],\n",
      "          [-3.1600, -1.5880],\n",
      "          [-7.0321, -3.6070],\n",
      "          [ 0.1309,  0.5647]],\n",
      "\n",
      "         [[-2.9604,  0.5082],\n",
      "          [-2.5408,  0.2734],\n",
      "          [-5.9208,  1.0163],\n",
      "          [ 0.5293,  0.5541]]],\n",
      "\n",
      "\n",
      "        [[[ 0.2174, -0.4873],\n",
      "          [ 0.0422, -1.2162],\n",
      "          [ 2.7725, -1.7794],\n",
      "          [ 0.2451, -0.5890]],\n",
      "\n",
      "         [[-0.3516, -0.1804],\n",
      "          [-1.0638, -0.6114],\n",
      "          [-2.6134,  0.7677],\n",
      "          [-0.7227,  0.0880]],\n",
      "\n",
      "         [[-0.2960,  0.0508],\n",
      "          [-1.1353,  0.5204],\n",
      "          [-4.3511, -4.5883],\n",
      "          [-0.8708, -0.3209]]]], grad_fn=<TransposeBackward0>)\n",
      "  Queries shape: torch.Size([2, 3, 4, 2])\n",
      "  Values shape: torch.Size([2, 3, 4, 2])\n",
      "\n",
      "4. ATTENTION SCORES COMPUTATION\n",
      "------------------------------\n",
      "Computing Q @ K^T for each head:\n",
      "  queries shape: torch.Size([2, 3, 4, 2])\n",
      "  keys.transpose(2,3) shape: torch.Size([2, 3, 2, 4])\n",
      "Attention scores shape: torch.Size([2, 3, 4, 4])\n",
      "Scaling factor (sqrt(head_dim)): 1.4142135623730951\n",
      "\n",
      "5. CAUSAL MASK APPLICATION\n",
      "------------------------------\n",
      "Mask shape: torch.Size([4, 4])\n",
      "Mask (True=masked, False=allowed):\n",
      "tensor([[False,  True,  True,  True],\n",
      "        [False, False,  True,  True],\n",
      "        [False, False, False,  True],\n",
      "        [False, False, False, False]])\n",
      "\n",
      "Before masking - attention scores (first head of first batch):\n",
      "tensor([[ -5.4819,  -4.9821, -10.9638,  -0.6850],\n",
      "        [ -5.5110,  -5.0667, -11.0220,  -0.7467],\n",
      "        [-10.9638,  -9.9641, -21.9276,  -1.3701],\n",
      "        [ -3.8906,  -3.5534,  -7.7813,  -0.5036]], grad_fn=<SelectBackward0>)\n",
      "After masking - attention scores (first head of first batch):\n",
      "tensor([[ -5.4819,     -inf,     -inf,     -inf],\n",
      "        [ -5.5110,  -5.0667,     -inf,     -inf],\n",
      "        [-10.9638,  -9.9641, -21.9276,     -inf],\n",
      "        [ -3.8906,  -3.5534,  -7.7813,  -0.5036]], grad_fn=<SelectBackward0>)\n",
      "\n",
      "6. SOFTMAX AND DROPOUT\n",
      "------------------------------\n",
      "Attention weights shape: torch.Size([2, 3, 4, 4])\n",
      "Attention weights (first head of first batch):\n",
      "tensor([[1.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
      "        [4.2210e-01, 5.7790e-01, 0.0000e+00, 0.0000e+00],\n",
      "        [3.3024e-01, 6.6962e-01, 1.4188e-04, 0.0000e+00],\n",
      "        [7.5181e-02, 9.5430e-02, 4.8009e-03, 8.2459e-01]],\n",
      "       grad_fn=<SelectBackward0>)\n",
      "After dropout applied\n",
      "\n",
      "7. WEIGHTED VALUES COMPUTATION\n",
      "------------------------------\n",
      "Computing attention_weights @ values:\n",
      "  attention_weights shape: torch.Size([2, 3, 4, 4])\n",
      "  values shape: torch.Size([2, 3, 4, 2])\n",
      "Context vector after transpose: torch.Size([2, 4, 3, 2])\n",
      "\n",
      "8. COMBINE HEADS (RESHAPE)\n",
      "------------------------------\n",
      "Reshaping from torch.Size([2, 4, 3, 2]) to (b, num_tokens, d_out)\n",
      "Combined heads shape: torch.Size([2, 4, 6])\n",
      "9. OUTPUT PROJECTION\n",
      "------------------------------\n",
      "Final output shape: torch.Size([2, 4, 6])\n",
      "\n",
      "Output shape: torch.Size([2, 4, 6])\n",
      "Output tensor:\n",
      "tensor([[[ 1.1950e+00, -1.3262e+00, -3.1353e+00, -8.2177e-01,  3.0350e+00,\n",
      "           9.3315e-01],\n",
      "         [ 1.1610e+00, -1.2722e+00, -2.9787e+00, -8.1240e-01,  2.9398e+00,\n",
      "           8.1098e-01],\n",
      "         [ 6.1547e-01, -2.3427e+00, -2.2741e+00, -2.4149e+00,  1.9907e+00,\n",
      "          -6.8512e-01],\n",
      "         [ 2.7735e+00, -3.8618e+00, -8.0292e+00, -2.1373e+00,  6.1572e+00,\n",
      "           2.8062e+00]],\n",
      "\n",
      "        [[ 7.4261e-02,  4.3630e-02, -4.8358e-01, -1.2224e-01,  2.7152e-01,\n",
      "           6.3890e-03],\n",
      "         [ 1.6122e-01,  2.7033e-01, -5.4895e-01,  2.0298e-01,  3.7068e-01,\n",
      "           3.3894e-01],\n",
      "         [-1.2856e-01,  2.0444e-01, -1.2735e+00,  1.8988e-03,  1.6949e-01,\n",
      "          -4.5503e-01],\n",
      "         [ 3.6849e-02,  6.3211e-02, -1.1565e+00, -7.4961e-02,  4.9722e-01,\n",
      "          -2.1288e-01]]], grad_fn=<ViewBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# import sys\n",
    "# import os\n",
    "# sys.path.append('./lib')\n",
    "# from multihead_attention import MultiHeadAttention\n",
    "# import torch\n",
    "\n",
    "# Create example with specified parameters\n",
    "mha = MultiHeadAttention(\n",
    "    d_in=6,\n",
    "    d_out=6,\n",
    "    context_length=4,\n",
    "    num_heads=3,\n",
    "    dropout=0.1,\n",
    "    qkv_bias=False\n",
    ")\n",
    "\n",
    "torch.manual_seed(42)\n",
    "# Create sample input data (batch_size=2, sequence_length=4, d_in=6)\n",
    "sample_input = torch.tensor([\n",
    "    # Batch 1\n",
    "    [\n",
    "        [1.0, 2.0, 3.0, 4.0, 5.0, 6.0],    # Token 1\n",
    "        [0.5, 1.5, 2.5, 3.5, 4.5, 5.5],    # Token 2\n",
    "        [2.0, 4.0, 6.0, 8.0, 10.0, 12.0],  # Token 3\n",
    "        [1.0, 0.0, -1.0, 2.0, 3.0, -2.0]   # Token 4\n",
    "    ],\n",
    "    # Batch 2\n",
    "    [\n",
    "        [0.1, 0.2, 0.3, 0.4, 0.5, 0.6],    # Token 1\n",
    "        [1.1, 1.2, 1.3, 1.4, 1.5, 1.6],    # Token 2\n",
    "        [-1.0, 2.0, -3.0, 4.0, -5.0, 6.0], # Token 3\n",
    "        [0.0, 1.0, 0.0, 1.0, 0.0, 1.0]     # Token 4\n",
    "    ]\n",
    "])\n",
    "\n",
    "print(\"Input shape:\", sample_input.shape)\n",
    "print(\"Input tensor:\")\n",
    "print(sample_input)\n",
    "print()\n",
    "\n",
    "# Forward pass through MultiHeadAttention\n",
    "output = mha(sample_input)\n",
    "print(\"Output shape:\", output.shape)\n",
    "print(\"Output tensor:\")\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4d55fac",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "3.12.11",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
