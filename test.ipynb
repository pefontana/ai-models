{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "78558224",
   "metadata": {},
   "source": [
    "# DataLoader Tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a8cc03ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Testing DataLoader with different stride values ===\n",
      "\n",
      "--- STRIDE = 5 ---\n",
      "Dataset size: 13 sequences\n",
      "Number of batches: 6\n",
      "Batch 0:\n",
      "  Input shape: torch.Size([2, 20])\n",
      "  Input tokens (first seq): [198, 818, 262, 995, 286, 11666, 4430, 11, 3303, 4981, 423, 5854, 1143, 703, 356, 1429, 290, 7716, 2420, 13]...\n",
      "  Target tokens (first seq): [818, 262, 995, 286, 11666, 4430, 11, 3303, 4981, 423, 5854, 1143, 703, 356, 1429, 290, 7716, 2420, 13, 220]...\n",
      "  Target text (first seq): In the world of artificial intelligence, language models have revolutionized how we process and generate text. \n",
      "  Input tokens (second seq): [11666, 4430, 11, 3303, 4981, 423, 5854, 1143, 703, 356]...\n",
      "  Target tokens (second seq): [4430, 11, 3303, 4981, 423, 5854, 1143, 703, 356, 1429]...\n",
      "\n",
      "Batch 1:\n",
      "  Input shape: torch.Size([2, 20])\n",
      "  Input tokens (first seq): [423, 5854, 1143, 703, 356, 1429, 290, 7716, 2420, 13, 220, 198, 4711, 3341, 779, 45619, 588, 6121, 364, 284]...\n",
      "  Target tokens (first seq): [5854, 1143, 703, 356, 1429, 290, 7716, 2420, 13, 220, 198, 4711, 3341, 779, 45619, 588, 6121, 364, 284, 2193]...\n",
      "  Target text (first seq):  revolutionized how we process and generate text. \n",
      "These systems use architectures like transformers to learn\n",
      "  Input tokens (second seq): [1429, 290, 7716, 2420, 13, 220, 198, 4711, 3341, 779]...\n",
      "  Target tokens (second seq): [290, 7716, 2420, 13, 220, 198, 4711, 3341, 779, 45619]...\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "sys.path.append('./lib')\n",
    "\n",
    "from dataloader import create_dataloader\n",
    "import torch\n",
    "import tiktoken\n",
    "tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
    "\n",
    "# Test text\n",
    "sample_text = \"\"\"\n",
    "In the world of artificial intelligence, language models have revolutionized how we process and generate text. \n",
    "These systems use architectures like transformers to learn complex patterns in textual data.\n",
    "The tokenization process converts text into numbers that the model can process efficiently.\n",
    "DataLoaders allow loading and processing data in batches during training.\n",
    "The sliding window technique with stride helps create overlapping sequences for better learning.\n",
    "\"\"\"\n",
    "\n",
    "print(\"=== Testing DataLoader with different stride values ===\\n\")\n",
    "\n",
    "# Base configuration\n",
    "batch_size = 2\n",
    "max_length = 20\n",
    "\n",
    "# Test with different stride values\n",
    "strides = [5]\n",
    "\n",
    "for stride in strides:\n",
    "    print(f\"--- STRIDE = {stride} ---\")\n",
    "    \n",
    "    # Create DataLoader\n",
    "    dataloader = create_dataloader(\n",
    "        text=sample_text,\n",
    "        batch_size=batch_size,\n",
    "        max_length=max_length,\n",
    "        stride=stride,\n",
    "        shuffle=False \n",
    "    )\n",
    "    \n",
    "    print(f\"Dataset size: {len(dataloader.dataset)} sequences\")\n",
    "\n",
    "    print(f\"Number of batches: {len(dataloader)}\")\n",
    "    \n",
    "    # Show first 2 batches\n",
    "    for i, (inputs, targets) in enumerate(dataloader):\n",
    "        if i >= 2:  # Only show first 2 batches\n",
    "            break\n",
    "        print(f\"Batch {i}:\")\n",
    "        print(f\"  Input shape: {inputs.shape}\")\n",
    "        print(f\"  Input tokens (first seq): {inputs[0][:30].tolist()}...\")\n",
    "        print(f\"  Target tokens (first seq): {targets[0][:30].tolist()}...\")\n",
    "        print(f\"  Target text (first seq): {tokenizer.decode(targets[0].tolist())}\")\n",
    "        print(f\"  Input tokens (second seq): {inputs[1][:10].tolist()}...\")\n",
    "        print(f\"  Target tokens (second seq): {targets[1][:10].tolist()}...\")\n",
    "        print()\n",
    "    \n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e4d1917c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Batch 0 ---\n",
      "Batch size: 2\n",
      "Sequence length: 20\n",
      "Token embeddings shape: torch.Size([2, 20, 768])\n",
      "tensor([[[-6.6324e-01,  4.1704e-01, -1.2894e+00,  ..., -1.8791e+00,\n",
      "           8.2641e-01,  9.7305e-01],\n",
      "         [-1.4581e+00,  7.4356e-01, -5.9911e-02,  ...,  1.6649e+00,\n",
      "          -9.2707e-01,  1.4055e+00],\n",
      "         [ 3.9818e-01,  2.1474e+00, -1.0382e+00,  ...,  1.3475e-03,\n",
      "          -8.9302e-01,  1.7736e+00],\n",
      "         ...,\n",
      "         [ 1.1110e+00,  1.2616e+00, -5.4483e-01,  ..., -1.9394e-01,\n",
      "           1.6118e+00, -6.3609e-01],\n",
      "         [ 2.0582e+00, -2.6760e-01,  1.7627e+00,  ..., -1.1155e+00,\n",
      "          -1.6484e+00, -2.1745e-01],\n",
      "         [ 5.8419e-01, -2.8842e-01, -1.4957e+00,  ...,  1.3730e+00,\n",
      "           1.8477e+00,  1.3674e+00]],\n",
      "\n",
      "        [[-6.2240e-01,  5.3057e-01, -3.9224e-02,  ...,  2.1672e+00,\n",
      "           2.1350e-01, -3.8101e-02],\n",
      "         [-9.7485e-01,  1.0174e+00,  1.0718e+00,  ...,  1.0746e+00,\n",
      "          -3.1815e-01, -1.4813e+00],\n",
      "         [-3.8978e-01,  9.1824e-02, -5.8360e-01,  ...,  2.9924e-01,\n",
      "          -1.4014e+00,  6.8730e-01],\n",
      "         ...,\n",
      "         [ 1.4771e+00, -2.8280e-01,  2.5644e-01,  ...,  2.6731e-01,\n",
      "          -1.7842e-02,  1.1666e+00],\n",
      "         [-2.8270e-01, -2.2961e-01,  6.0463e-01,  ..., -8.8644e-01,\n",
      "          -2.6968e-01,  3.7722e-01],\n",
      "         [ 1.7432e-01, -1.5138e+00, -1.3074e-01,  ..., -2.2817e+00,\n",
      "          -5.1752e-01,  1.3801e+00]]], grad_fn=<EmbeddingBackward0>)\n",
      "Position indices: tensor([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17,\n",
      "        18, 19])\n",
      "Position embeddings shape: torch.Size([20, 768])\n",
      "tensor([[ 0.1760,  0.0170, -0.9972,  ..., -1.3285, -0.5761, -1.1505],\n",
      "        [-0.5323,  0.2023,  0.3931,  ..., -1.0997, -0.6090, -0.8760],\n",
      "        [ 0.1948, -0.1769,  1.1219,  ..., -0.7842, -1.0247, -0.4120],\n",
      "        ...,\n",
      "        [ 1.7075, -1.1472,  1.4318,  ..., -0.9895, -0.2923,  0.6608],\n",
      "        [-0.2243,  0.1585, -0.7132,  ..., -0.0292,  0.5849, -1.0631],\n",
      "        [ 0.4371,  0.6910, -0.5455,  ...,  0.6139,  2.2771, -0.2712]],\n",
      "       grad_fn=<EmbeddingBackward0>)\n",
      "Final result shape: torch.Size([2, 20, 768])\n",
      "tensor([[[-0.5414,  0.4823, -0.0000,  ..., -3.5641,  0.2782, -0.1971],\n",
      "         [-0.0000,  1.0510,  0.3702,  ...,  0.6280, -1.7068,  0.5883],\n",
      "         [ 0.6589,  2.1894,  0.0929,  ..., -0.8699, -2.1308,  1.5129],\n",
      "         ...,\n",
      "         [ 3.1316,  0.1271,  0.9855,  ..., -1.3150,  1.4660,  0.0275],\n",
      "         [ 2.0376, -0.1212,  1.1662,  ..., -1.2719, -0.0000, -1.4228],\n",
      "         [ 1.1347,  0.4473, -2.2681,  ...,  2.2077,  4.5831,  1.2181]],\n",
      "\n",
      "        [[-0.4960,  0.6085, -1.1516,  ...,  0.9318, -0.0000, -1.3206],\n",
      "         [-1.6746,  1.3553,  1.6277,  ..., -0.0279, -0.0000, -2.6192],\n",
      "         [-0.2166, -0.0945,  0.5981,  ..., -0.5389, -2.6957,  0.3059],\n",
      "         ...,\n",
      "         [ 0.0000, -1.5889,  1.8758,  ..., -0.8025, -0.3447,  2.0305],\n",
      "         [-0.0000, -0.0790, -0.1206,  ..., -1.0174,  0.3502, -0.7620],\n",
      "         [ 0.6793, -0.9142, -0.7514,  ..., -1.8531,  1.9551,  1.2321]]],\n",
      "       grad_fn=<MulBackward0>)\n",
      "\n",
      "--- Batch 1 ---\n",
      "Batch size: 2\n",
      "Sequence length: 20\n",
      "Token embeddings shape: torch.Size([2, 20, 768])\n",
      "tensor([[[-0.0208, -0.6788, -1.7014,  ...,  0.1783,  0.9536,  0.3049],\n",
      "         [-0.3542, -0.4140, -1.2091,  ..., -0.4797, -0.3297, -1.4632],\n",
      "         [ 1.1181,  0.1444,  1.8659,  ..., -0.8644,  0.6964,  0.8186],\n",
      "         ...,\n",
      "         [-0.4526, -0.4497,  0.3842,  ...,  0.0229,  0.2592,  0.0252],\n",
      "         [-0.0647,  1.0285, -0.8235,  ..., -0.8624,  0.0378,  0.7753],\n",
      "         [-1.8180, -1.1525,  0.2698,  ..., -1.2725,  1.2435,  0.9105]],\n",
      "\n",
      "        [[ 0.2165, -0.4463,  1.0130,  ...,  2.1782, -0.5957,  0.1589],\n",
      "         [-0.0153, -0.4694, -0.6934,  ..., -0.3194,  0.6714,  0.1023],\n",
      "         [ 1.1110,  1.2616, -0.5448,  ..., -0.1939,  1.6118, -0.6361],\n",
      "         ...,\n",
      "         [-0.7392, -1.7600, -0.2340,  ...,  0.0709,  0.8066, -0.8100],\n",
      "         [-1.0789, -1.0106,  0.5592,  ...,  0.4203,  0.7825,  1.4608],\n",
      "         [-0.7658, -0.3874, -0.8689,  ..., -1.6672,  0.4899,  1.6354]]],\n",
      "       grad_fn=<EmbeddingBackward0>)\n",
      "Position indices: tensor([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17,\n",
      "        18, 19])\n",
      "Position embeddings shape: torch.Size([20, 768])\n",
      "tensor([[ 0.1760,  0.0170, -0.9972,  ..., -1.3285, -0.5761, -1.1505],\n",
      "        [-0.5323,  0.2023,  0.3931,  ..., -1.0997, -0.6090, -0.8760],\n",
      "        [ 0.1948, -0.1769,  1.1219,  ..., -0.7842, -1.0247, -0.4120],\n",
      "        ...,\n",
      "        [ 1.7075, -1.1472,  1.4318,  ..., -0.9895, -0.2923,  0.6608],\n",
      "        [-0.2243,  0.1585, -0.7132,  ..., -0.0292,  0.5849, -1.0631],\n",
      "        [ 0.4371,  0.6910, -0.5455,  ...,  0.6139,  2.2771, -0.2712]],\n",
      "       grad_fn=<EmbeddingBackward0>)\n",
      "Final result shape: torch.Size([2, 20, 768])\n",
      "tensor([[[ 0.1724, -0.7353, -2.9985,  ..., -0.0000,  0.0000, -0.9396],\n",
      "         [-0.9849, -0.2352, -0.9066,  ..., -1.7549, -1.0430, -2.5991],\n",
      "         [ 1.4588, -0.0361,  0.0000,  ..., -1.8318, -0.3648,  0.4518],\n",
      "         ...,\n",
      "         [ 1.3943, -1.7744,  2.0177,  ..., -1.0740, -0.0369,  0.7623],\n",
      "         [-0.3211,  1.3190, -1.7074,  ..., -0.0000,  0.6918, -0.3198],\n",
      "         [-1.5343, -0.5128, -0.3063,  ..., -0.7318,  3.9118,  0.7104]],\n",
      "\n",
      "        [[ 0.4360, -0.4769,  0.0000,  ...,  0.9441, -1.3020, -0.0000],\n",
      "         [-0.6084, -0.2968, -0.0000,  ..., -1.5768,  0.0693, -0.8597],\n",
      "         [ 1.4509,  1.2053,  0.6411,  ..., -1.0868,  0.6523, -0.0000],\n",
      "         ...,\n",
      "         [ 1.0759, -3.2302,  1.3308,  ..., -1.0206,  0.5714, -0.1657],\n",
      "         [-1.4480, -0.9467, -0.1711,  ...,  0.4346,  1.5193,  0.4420],\n",
      "         [-0.3653,  0.0000, -0.0000,  ..., -1.1703,  3.0744,  1.5158]]],\n",
      "       grad_fn=<MulBackward0>)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "GPT_CONFIG_124M = {\n",
    "    \"vocab_size\": 50257,   # Vocabulary size\n",
    "    \"context_length\": 256, # Shortened context length (orig: 1024)\n",
    "    \"emb_dim\": 768,        # Embedding dimension\n",
    "    \"n_heads\": 12,         # Number of attention heads\n",
    "    \"n_layers\": 12,        # Number of layers\n",
    "    \"drop_rate\": 0.1,      # Dropout rate\n",
    "    \"qkv_bias\": False      # Query-key-value bias\n",
    "}\n",
    "\n",
    "\n",
    "tok_emb = torch.nn.Embedding(GPT_CONFIG_124M[\"vocab_size\"], GPT_CONFIG_124M[\"emb_dim\"])\n",
    "pos_emb = torch.nn.Embedding(GPT_CONFIG_124M[\"context_length\"], GPT_CONFIG_124M[\"emb_dim\"])\n",
    "drop_emb = torch.nn.Dropout(GPT_CONFIG_124M[\"drop_rate\"])\n",
    "\n",
    "\n",
    "for i, (inputs, targets) in enumerate(dataloader):\n",
    "    if i >= 2:  \n",
    "        break\n",
    "\n",
    "    batch_size, seq_len = inputs.shape\n",
    "    print(f\"--- Batch {i} ---\")\n",
    "    print(f\"Batch size: {batch_size}\")\n",
    "    print(f\"Sequence length: {seq_len}\")\n",
    "\n",
    "    tok_embeds = tok_emb(inputs)\n",
    "    print(f\"Token embeddings shape: {tok_embeds.shape}\")\n",
    "    print(tok_embeds)\n",
    "\n",
    "    pos_indices = torch.arange(seq_len)\n",
    "    print(f\"Position indices: {pos_indices}\")\n",
    "    pos_embeds = pos_emb(pos_indices)\n",
    "    print(f\"Position embeddings shape: {pos_embeds.shape}\")\n",
    "    print(pos_embeds)\n",
    "\n",
    "    final_embeds = tok_embeds + pos_embeds\n",
    "    result = drop_emb(final_embeds)\n",
    "    print(f\"Final result shape: {result.shape}\")\n",
    "    print(result)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c3511a0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "3.12.11",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
