{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "78558224",
   "metadata": {},
   "source": [
    "# DataLoader Tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8cc03ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "sys.path.append('./lib')\n",
    "\n",
    "from dataloader import create_dataloader\n",
    "import torch\n",
    "import tiktoken\n",
    "tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
    "\n",
    "# Test text\n",
    "sample_text = \"\"\"\n",
    "In the world of artificial intelligence, language models have revolutionized how we process and generate text. \n",
    "These systems use architectures like transformers to learn complex patterns in textual data.\n",
    "The tokenization process converts text into numbers that the model can process efficiently.\n",
    "DataLoaders allow loading and processing data in batches during training.\n",
    "The sliding window technique with stride helps create overlapping sequences for better learning.\n",
    "\"\"\"\n",
    "\n",
    "print(\"=== Testing DataLoader with different stride values ===\\n\")\n",
    "\n",
    "# Base configuration\n",
    "batch_size = 2\n",
    "max_length = 20\n",
    "\n",
    "# Test with different stride values\n",
    "strides = [1,5]\n",
    "\n",
    "for stride in strides:\n",
    "    print(f\"--- STRIDE = {stride} ---\")\n",
    "    \n",
    "    # Create DataLoader\n",
    "    dataloader = create_dataloader(\n",
    "        text=sample_text,\n",
    "        batch_size=batch_size,\n",
    "        max_length=max_length,\n",
    "        stride=stride,\n",
    "        shuffle=False \n",
    "    )\n",
    "    \n",
    "    print(f\"Dataset size: {len(dataloader.dataset)} sequences\")\n",
    "\n",
    "    print(f\"Number of batches: {len(dataloader)}\")\n",
    "    \n",
    "    # Show first 2 batches\n",
    "    for i, (inputs, targets) in enumerate(dataloader):\n",
    "        if i >= 2:  # Only show first 2 batches\n",
    "            break\n",
    "        print(f\"Batch {i}:\")\n",
    "        print(f\"  Input shape: {inputs.shape}\")\n",
    "        print(f\"  Input tokens (first seq): {inputs[0][:30].tolist()}...\")\n",
    "        print(f\"  Target tokens (first seq): {targets[0][:30].tolist()}...\")\n",
    "        print(f\"  Target text (first seq): {tokenizer.decode(targets[0].tolist())}\")\n",
    "        print(f\"  Input tokens (second seq): {inputs[1][:10].tolist()}...\")\n",
    "        print(f\"  Target tokens (second seq): {targets[1][:10].tolist()}...\")\n",
    "        print()\n",
    "    \n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4d1917c",
   "metadata": {},
   "outputs": [],
   "source": [
    "GPT_CONFIG_124M = {\n",
    "    \"vocab_size\": 50257,   # Vocabulary size\n",
    "    \"context_length\": 256, # Shortened context length (orig: 1024)\n",
    "    \"emb_dim\": 768,        # Embedding dimension\n",
    "    \"n_heads\": 12,         # Number of attention heads\n",
    "    \"n_layers\": 12,        # Number of layers\n",
    "    \"drop_rate\": 0.1,      # Dropout rate\n",
    "    \"qkv_bias\": False      # Query-key-value bias\n",
    "}\n",
    "\n",
    "\n",
    "tok_emb = torch.nn.Embedding(GPT_CONFIG_124M[\"vocab_size\"], GPT_CONFIG_124M[\"emb_dim\"])\n",
    "pos_emb = torch.nn.Embedding(GPT_CONFIG_124M[\"context_length\"], GPT_CONFIG_124M[\"emb_dim\"])\n",
    "drop_emb = torch.nn.Dropout(GPT_CONFIG_124M[\"drop_rate\"])\n",
    "\n",
    "\n",
    "for i, (inputs, targets) in enumerate(dataloader):\n",
    "    if i >= 2:  \n",
    "        break\n",
    "\n",
    "    batch_size, seq_len = inputs.shape\n",
    "    print(f\"--- Batch {i} ---\")\n",
    "    print(f\"Batch size: {batch_size}\")\n",
    "    print(f\"Sequence length: {seq_len}\")\n",
    "\n",
    "    tok_embeds = tok_emb(inputs)\n",
    "    print(f\"Token embeddings shape: {tok_embeds.shape}\")\n",
    "    print(tok_embeds)\n",
    "\n",
    "    pos_indices = torch.arange(seq_len)\n",
    "    print(f\"Position indices: {pos_indices}\")\n",
    "    pos_embeds = pos_emb(pos_indices)\n",
    "    print(f\"Position embeddings shape: {pos_embeds.shape}\")\n",
    "    print(pos_embeds)\n",
    "\n",
    "    final_embeds = tok_embeds + pos_embeds\n",
    "    result = drop_emb(final_embeds)\n",
    "    print(f\"Final result shape: {result.shape}\")\n",
    "    print(result)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f3a6150",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, d_in, d_out, context_length, dropout, num_heads, qkv_bias=False):\n",
    "        super().__init__()\n",
    "        assert d_out % num_heads == 0, \"d_out must be divisible by n_heads\"\n",
    "\n",
    "        self.d_out = d_out\n",
    "        self.num_heads = num_heads\n",
    "        self.head_dim = d_out // num_heads  # Reduce the projection dim to match desired output dim\n",
    "\n",
    "        self.W_query = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "        self.W_key = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "        self.W_value = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "        self.out_proj = nn.Linear(d_out, d_out)  # Linear layer to combine head outputs\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.register_buffer('mask', torch.triu(torch.ones(context_length, context_length), diagonal=1))\n",
    "\n",
    "    def forward(self, x):\n",
    "        print(\"=\" * 50)\n",
    "        print(\"MULTIHEAD ATTENTION FORWARD PASS\")\n",
    "        print(\"=\" * 50)\n",
    "        print(\"W_key weight matrix:\")\n",
    "        print(self.W_key.weight)\n",
    "        \n",
    "        b, num_tokens, d_in = x.shape\n",
    "        print(f\"INPUT: batch_size={b}, num_tokens={num_tokens}, d_in={d_in}\")\n",
    "        print(f\"Input shape: {x.shape}\")\n",
    "        print(f\"Configuration: d_out={self.d_out}, num_heads={self.num_heads}, head_dim={self.head_dim}\")\n",
    "        print()\n",
    "\n",
    "        # Linear transformations\n",
    "        print(\"1. LINEAR TRANSFORMATIONS (Q, K, V)\")\n",
    "        print(\"-\" * 30)\n",
    "        print(\"W_key weight matrix:\")\n",
    "        print(self.W_key.weight)\n",
    "        keys = self.W_key(x)  # Shape: (b, num_tokens, d_out)\n",
    "        queries = self.W_query(x)\n",
    "        values = self.W_value(x)\n",
    "        print(f\"After linear layers:\")\n",
    "        print(f\"  Keys shape: {keys.shape}\")\n",
    "        print(f\"  Keys tensor:\")\n",
    "        print(keys)\n",
    "        print(f\"  Queries shape: {queries.shape}\")\n",
    "        print(f\"  Values shape: {values.shape}\")\n",
    "        print()\n",
    "\n",
    "        # We implicitly split the matrix by adding a `num_heads` dimension\n",
    "        # Unroll last dim: (b, num_tokens, d_out) -> (b, num_tokens, num_heads, head_dim)\n",
    "        print(\"2. VIEW OPERATION - SPLIT INTO HEADS\")\n",
    "        print(\"-\" * 30)\n",
    "        print(f\"Reshaping from (b, num_tokens, d_out) to (b, num_tokens, num_heads, head_dim)\")\n",
    "        print(f\"  d_out={self.d_out} = num_heads={self.num_heads} Ã— head_dim={self.head_dim}\")\n",
    "        \n",
    "        keys = keys.view(b, num_tokens, self.num_heads, self.head_dim)\n",
    "        values = values.view(b, num_tokens, self.num_heads, self.head_dim)\n",
    "        queries = queries.view(b, num_tokens, self.num_heads, self.head_dim)\n",
    "        \n",
    "        print(f\"After view operation:\")\n",
    "        print(f\"  Keys shape: {keys.shape}\")\n",
    "        print(f\"  Keys AW: {keys}\")\n",
    "        print(f\"  Queries shape: {queries.shape}\")\n",
    "        print(f\"  Values shape: {values.shape}\")\n",
    "        print()\n",
    "\n",
    "        # Transpose: (b, num_tokens, num_heads, head_dim) -> (b, num_heads, num_tokens, head_dim)\n",
    "        print(\"3. TRANSPOSE OPERATION\")\n",
    "        print(\"-\" * 30)\n",
    "        print(f\"Transposing dimensions 1 and 2:\")\n",
    "        print(f\"  (b, num_tokens, num_heads, head_dim) -> (b, num_heads, num_tokens, head_dim)\")\n",
    "        \n",
    "        keys = keys.transpose(1, 2)\n",
    "        queries = queries.transpose(1, 2)\n",
    "        values = values.transpose(1, 2)\n",
    "        \n",
    "        print(f\"After transpose:\")\n",
    "        print(f\"  Keys shape: {keys.shape}\")\n",
    "        print(f\"  Keys AT: {keys}\")\n",
    "        print(f\"  Queries shape: {queries.shape}\")\n",
    "        print(f\"  Values shape: {values.shape}\")\n",
    "        print()\n",
    "\n",
    "        # Compute scaled dot-product attention (aka self-attention) with a causal mask\n",
    "        print(\"4. ATTENTION SCORES COMPUTATION\")\n",
    "        print(\"-\" * 30)\n",
    "        print(f\"Computing Q @ K^T for each head:\")\n",
    "        print(f\"  queries shape: {queries.shape}\")\n",
    "        print(f\"  keys.transpose(2,3) shape: {keys.transpose(2, 3).shape}\")\n",
    "        \n",
    "        attn_scores = queries @ keys.transpose(2, 3)  # Dot product for each head\n",
    "        print(f\"Attention scores shape: {attn_scores.shape}\")\n",
    "        print(f\"Scaling factor (sqrt(head_dim)): {keys.shape[-1]**0.5}\")\n",
    "        print()\n",
    "\n",
    "        # Original mask truncated to the number of tokens and converted to boolean\n",
    "        print(\"5. CAUSAL MASK APPLICATION\")\n",
    "        print(\"-\" * 30)\n",
    "        mask_bool = self.mask.bool()[:num_tokens, :num_tokens]\n",
    "        print(f\"Mask shape: {mask_bool.shape}\")\n",
    "        print(f\"Mask (True=masked, False=allowed):\")\n",
    "        print(mask_bool)\n",
    "        print()\n",
    "\n",
    "        # Use the mask to fill attention scores\n",
    "        print(\"Before masking - attention scores (first head of first batch):\")\n",
    "        print(attn_scores[0, 0])\n",
    "        attn_scores.masked_fill_(mask_bool, -torch.inf)\n",
    "        print(\"After masking - attention scores (first head of first batch):\")\n",
    "        print(attn_scores[0, 0])\n",
    "        print()\n",
    "\n",
    "        print(\"6. SOFTMAX AND DROPOUT\")\n",
    "        print(\"-\" * 30)\n",
    "        attn_weights = torch.softmax(attn_scores / keys.shape[-1]**0.5, dim=-1)\n",
    "        print(f\"Attention weights shape: {attn_weights.shape}\")\n",
    "        print(\"Attention weights (first head of first batch):\")\n",
    "        print(attn_weights[0, 0])\n",
    "        \n",
    "        attn_weights = self.dropout(attn_weights)\n",
    "        print(\"After dropout applied\")\n",
    "        print()\n",
    "\n",
    "        # Shape: (b, num_tokens, num_heads, head_dim)\n",
    "        print(\"7. WEIGHTED VALUES COMPUTATION\")\n",
    "        print(\"-\" * 30)\n",
    "        print(f\"Computing attention_weights @ values:\")\n",
    "        print(f\"  attention_weights shape: {attn_weights.shape}\")\n",
    "        print(f\"  values shape: {values.shape}\")\n",
    "        \n",
    "        context_vec = (attn_weights @ values).transpose(1, 2)\n",
    "        print(f\"Context vector after transpose: {context_vec.shape}\")\n",
    "        print()\n",
    "\n",
    "        # Combine heads, where self.d_out = self.num_heads * self.head_dim\n",
    "        print(\"8. COMBINE HEADS (RESHAPE)\")\n",
    "        print(\"-\" * 30)\n",
    "        print(f\"Reshaping from {context_vec.shape} to (b, num_tokens, d_out)\")\n",
    "        context_vec = context_vec.reshape(b, num_tokens, self.d_out)\n",
    "        print(f\"Combined heads shape: {context_vec.shape}\")\n",
    "        \n",
    "        print(\"9. OUTPUT PROJECTION\")\n",
    "        print(\"-\" * 30)\n",
    "        context_vec = self.out_proj(context_vec)  # optional projection\n",
    "        print(f\"Final output shape: {context_vec.shape}\")\n",
    "        print()\n",
    "\n",
    "        return context_vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c3511a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import sys\n",
    "# import os\n",
    "# sys.path.append('./lib')\n",
    "# from multihead_attention import MultiHeadAttention\n",
    "# import torch\n",
    "\n",
    "# Create example with specified parameters\n",
    "mha = MultiHeadAttention(\n",
    "    d_in=6,\n",
    "    d_out=6,\n",
    "    context_length=4,\n",
    "    num_heads=3,\n",
    "    dropout=0.1,\n",
    "    qkv_bias=False\n",
    ")\n",
    "\n",
    "torch.manual_seed(42)\n",
    "# Create sample input data (batch_size=2, sequence_length=4, d_in=6)\n",
    "sample_input = torch.tensor([\n",
    "    # Batch 1\n",
    "    [\n",
    "        [1.0, 2.0, 3.0, 4.0, 5.0, 6.0],    # Token 1\n",
    "        [0.5, 1.5, 2.5, 3.5, 4.5, 5.5],    # Token 2\n",
    "        [2.0, 4.0, 6.0, 8.0, 10.0, 12.0],  # Token 3\n",
    "        [1.0, 0.0, -1.0, 2.0, 3.0, -2.0]   # Token 4\n",
    "    ],\n",
    "    # Batch 2\n",
    "    [\n",
    "        [0.1, 0.2, 0.3, 0.4, 0.5, 0.6],    # Token 1\n",
    "        [1.1, 1.2, 1.3, 1.4, 1.5, 1.6],    # Token 2\n",
    "        [-1.0, 2.0, -3.0, 4.0, -5.0, 6.0], # Token 3\n",
    "        [0.0, 1.0, 0.0, 1.0, 0.0, 1.0]     # Token 4\n",
    "    ]\n",
    "])\n",
    "\n",
    "print(\"Input shape:\", sample_input.shape)\n",
    "print(\"Input tensor:\")\n",
    "print(sample_input)\n",
    "print()\n",
    "\n",
    "# Forward pass through MultiHeadAttention\n",
    "output = mha(sample_input)\n",
    "print(\"Output shape:\", output.shape)\n",
    "print(\"Output tensor:\")\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4d55fac",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'main'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 7\u001b[39m\n\u001b[32m      5\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mdataloader\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m create_dataloader\n\u001b[32m      6\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mgpt_model\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m GPTModel\n\u001b[32m----> \u001b[39m\u001b[32m7\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mmain\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m calc_loss_batch\n\u001b[32m      8\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorch\u001b[39;00m\n\u001b[32m      9\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtiktoken\u001b[39;00m\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'main'"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "sys.path.append('./lib')\n",
    "\n",
    "from dataloader import create_dataloader\n",
    "from gpt_model import GPTModel, calc_loss_batch\n",
    "import torch\n",
    "import tiktoken\n",
    "tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
    "\n",
    "print(\"Starting GPT test...\")\n",
    "\n",
    "# Test text\n",
    "sample_text = \"\"\"\n",
    "In the world of artificial intelligence, language models have revolutionized how we process and generate text. \n",
    "These systems use architectures like transformers to learn complex patterns in textual data.\n",
    "The tokenization process converts text into numbers that the model can process efficiently.\n",
    "DataLoaders allow loading and processing data in batches during training.\n",
    "The sliding window technique with stride helps create overlapping sequences for better learning.\n",
    "In the world of artificial intelligence, language models have revolutionized how we process and generate text. \n",
    "These systems use architectures like transformers to learn complex patterns in textual data.\n",
    "The tokenization process converts text into numbers that the model can process efficiently.\n",
    "DataLoaders allow loading and processing data in batches during training.\n",
    "The sliding window technique with stride helps create overlapping sequences for better learning.\n",
    "In the world of artificial intelligence, language models have revolutionized how we process and generate text. \n",
    "These systems use architectures like transformers to learn complex patterns in textual data.\n",
    "The tokenization process converts text into numbers that the model can process efficiently.\n",
    "DataLoaders allow loading and processing data in batches during training.\n",
    "The sliding window technique with stride helps create overlapping sequences for better learning.\n",
    "In the world of artificial intelligence, language models have revolutionized how we process and generate text. \n",
    "These systems use architectures like transformers to learn complex patterns in textual data.\n",
    "The tokenization process converts text into numbers that the model can process efficiently.\n",
    "DataLoaders allow loading and processing data in batches during training.\n",
    "The sliding window technique with stride helps create overlapping sequences for better learning.\n",
    "In the world of artificial intelligence, language models have revolutionized how we process and generate text. \n",
    "These systems use architectures like transformers to learn complex patterns in textual data.\n",
    "The tokenization process converts text into numbers that the model can process efficiently.\n",
    "DataLoaders allow loading and processing data in batches during training.\n",
    "The sliding window technique with stride helps create overlapping sequences for better learning.\n",
    "In the world of artificial intelligence, language models have revolutionized how we process and generate text. \n",
    "These systems use architectures like transformers to learn complex patterns in textual data.\n",
    "The tokenization process converts text into numbers that the model can process efficiently.\n",
    "DataLoaders allow loading and processing data in batches during training.\n",
    "The sliding window technique with stride helps create overlapping sequences for better learning.\n",
    "In the world of artificial intelligence, language models have revolutionized how we process and generate text. \n",
    "These systems use architectures like transformers to learn complex patterns in textual data.\n",
    "The tokenization process converts text into numbers that the model can process efficiently.\n",
    "DataLoaders allow loading and processing data in batches during training.\n",
    "The sliding window technique with stride helps create overlapping sequences for better learning.\n",
    "In the world of artificial intelligence, language models have revolutionized how we process and generate text. \n",
    "These systems use architectures like transformers to learn complex patterns in textual data.\n",
    "The tokenization process converts text into numbers that the model can process efficiently.\n",
    "DataLoaders allow loading and processing data in batches during training.\n",
    "The sliding window technique with stride helps create overlapping sequences for better learning.\n",
    "In the world of artificial intelligence, language models have revolutionized how we process and generate text. \n",
    "These systems use architectures like transformers to learn complex patterns in textual data.\n",
    "The tokenization process converts text into numbers that the model can process efficiently.\n",
    "DataLoaders allow loading and processing data in batches during training.\n",
    "The sliding window technique with stride helps create overlapping sequences for better learning.\n",
    "In the world of artificial intelligence, language models have revolutionized how we process and generate text. \n",
    "These systems use architectures like transformers to learn complex patterns in textual data.\n",
    "The tokenization process converts text into numbers that the model can process efficiently.\n",
    "DataLoaders allow loading and processing data in batches during training.\n",
    "The sliding window technique with stride helps create overlapping sequences for better learning.\n",
    "In the world of artificial intelligence, language models have revolutionized how we process and generate text. \n",
    "These systems use architectures like transformers to learn complex patterns in textual data.\n",
    "The tokenization process converts text into numbers that the model can process efficiently.\n",
    "DataLoaders allow loading and processing data in batches during training.\n",
    "The sliding window technique with stride helps create overlapping sequences for better learning.\n",
    "In the world of artificial intelligence, language models have revolutionized how we process and generate text. \n",
    "These systems use architectures like transformers to learn complex patterns in textual data.\n",
    "The tokenization process converts text into numbers that the model can process efficiently.\n",
    "DataLoaders allow loading and processing data in batches during training.\n",
    "The sliding window technique with stride helps create overlapping sequences for better learning.\n",
    "\"\"\"\n",
    "\n",
    "GPT_CONFIG_124M = {\n",
    "    \"vocab_size\": 50257,   # Vocabulary size\n",
    "    \"context_length\": 256, # Shortened context length (orig: 1024)\n",
    "    \"emb_dim\": 768,        # Embedding dimension\n",
    "    \"n_heads\": 12,         # Number of attention heads\n",
    "    \"n_layers\": 12,        # Number of layers\n",
    "    \"drop_rate\": 0.1,      # Dropout rate\n",
    "    \"qkv_bias\": False      # Query-key-value bias\n",
    "}\n",
    "\n",
    "print(\"Creating GPT model...\")\n",
    "model = GPTModel(GPT_CONFIG_124M)\n",
    "print(\"Model created successfully!\")\n",
    "\n",
    "print(\"Creating dataloader...\")\n",
    "dataloader = create_dataloader(\n",
    "    text=sample_text,\n",
    "    batch_size=1,\n",
    "    max_length=256,  \n",
    "    stride=1,      \n",
    "    shuffle=False \n",
    ")\n",
    "print(f\"Dataloader created! Dataset size: {len(dataloader.dataset)}\")\n",
    "print(f\"Number of batches: {len(dataloader)}\")\n",
    "\n",
    "print(\"Starting forward pass...\")\n",
    "for i, (inputs, targets) in enumerate(dataloader):\n",
    "    if i >= 10:  # Only process first batch\n",
    "        break\n",
    "    print(f\"Processing Batch {i}:\")\n",
    "    print(f\"Input shape: {inputs.shape}\")\n",
    "    \n",
    "    try:\n",
    "        # a = inputs[0].tolist()\n",
    "        # prompt_str = tokenizer.decode(a)\n",
    "        # # print(\"prompt_str\", prompt_str)\n",
    "        # logits = model.forward(inputs)\n",
    "        # print(\"logits:\", logits)\n",
    "        # print(\"logits shape \", logits.shape)\n",
    "        # predicted_tokens_id = torch.argmax(logits, dim=-1)\n",
    "        # print(\"predicted_tokens_id shape\", predicted_tokens_id.shape)\n",
    "        # print(\"predicted_tokens_id\", predicted_tokens_id)\n",
    "        # print(\"targets.shape\", targets.shape)\n",
    "        \n",
    "        loss = calc_loss_batch(inputs,targets,model, inputs.device)\n",
    "        print(\"loss\", loss)\n",
    "    except Exception as e:\n",
    "        print(f\"Error during forward pass: {e}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f30d3193",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "3.12.11",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
